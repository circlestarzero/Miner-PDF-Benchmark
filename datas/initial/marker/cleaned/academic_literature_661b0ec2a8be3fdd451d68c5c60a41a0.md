# Minimum Stationary Values Of Sparse Random Directed Graphs Xing Shi Cai* And Guillem Perarnau**

**Mathematics Department, Uppsala University, Sweden. Email:* xingshi.cai@tutanota.com.

**Departament de Matem`atiques (MAT), Universitat Polit`ecnica de Catalunya (UPC), Barcelona, Spain.

Email: guillem.perarnau@upc.edu.

November 2, 2021

## Abstract

We consider the stationary distribution of the simple random walk on the directed configuration model with bounded degrees. Provided that the minimum out-degree is at least 2, with high probability (whp) there is a unique stationary distribution (unicity regime). We show that the minimum positive stationary value is whp n
−(1+C+o(1)) for some constant C ≥ 0 determined by the degree distribution, answering a question raised by Bordenave, Caputo and Salez [5]. In particular, C is the competing combination of two factors: (1) the contribution of atypically
"thin" in-neighbourhoods, controlled by subcritical branching processes; and (2) the contribution of atypically "light" trajectories, controlled by large deviation rate functions. Additionally, we give estimates for the expected lower tail of the empirical stationary distribution. As a byproduct of our proof, we obtain that the maximal hitting and the cover time are both n 1+C+o(1)
whp. Our results are in sharp contrast to those of Caputo and Quattropani [11] who showed that under the additional condition of minimum in-degree at least 2 (ergodicity regime), stationary values only have logarithmic fluctuations around n
−1.

## 1 Introduction 1.1 The Directed Configuration Model

The directed configuration model was introduced by Cooper and Frieze in [14]. Let [n] := {1*, . . . , n*}
be a set of n vertices. Let ~dn = ((d
− 1
, d+
1
)*, . . . ,*(d
− n
, d+
n P
)) be a bi-degree sequence with m :=
i∈[n]
d
+
i =Pi∈[n]
d
−
i
. Let δ
± and ∆± be the minimum and maximum in/out-degree, respectively.

The directed configuration model, which we denote by G~n = G~n(
~dn), is the random directed multigraph on [n] generated by giving d
−
i*heads* (in-half-edges) and d
+
i*tails* (out-half-edges) to vertex i, and then pairing the heads and the tails uniformly at random.

The directed configuration model is of practical importance as many complex real-world networks are directed. For instance, it has been used to study neural networks [2], Google's PageRank algorithm [13], and social networks [19].

The original paper by Cooper and Frieze [14] studies the birth of a linear size strongly connected component (scc) in G~n. Their result was recently improved by Graf [17] and by the two authors [8].

Lately, there has been some progress on the distances in the directed configuration model for bi-degree sequences with finite covariances. Typical distances in G~n were studied by van der Hoorn and Olvera-Cravioto [26]. Caputo and Quattropani [11] showed that the diameter of G~n is asymptotically equal to the typical distance, provided that δ
± ≥ 2 and ∆± = O(1). In our previous work [9], we showed that the diameter has different behaviour if no constraints on the minimum degree are imposed.

One motivation to the study of distances in G~n is its close connection to certain properties of random walks, in particular, to their stationary distribution, denoted by π. While π is trivially determined by the degree sequence in undirected graphs, in the directed case π is a complicated random measure that depends on the geometry of the random digraph. Cooper and Frieze [15] initiated the study of π in random digraphs, determining it on the strong connectivity regime of the directed Erd˝os-R´enyi random graph. They also established a relation between the minimum stationary value and stopping times such as the hitting and the cover time. Extremal stationary values for the r-out random digraph were studied by Addario-Berry, Balle, and the second author [1].

Regarding the directed configuration model, Bordenave, Caputo and Salez [5, 6] studied the mixing time of a random walk on G~n and showed that it exhibits cutoff. Additionally, they proved that for a vertex i ∈ [n], π(i) is essentially determined by the local in-neighbourhood of i and wellapproximated by a deterministic law (see Remark 1.8). These results provide a precise description of typical stationary probabilities but fall short to capture the exceptional values of π.

In [5], the authors raised the question of studying the extremal values of the stationary distribution in G~n. Let πmin and πmax be the smallest and largest positive values of π, respectively. From now on and throughout this paper, we will assume that all degrees are bounded; i.e., ∆± = O(1). In this context, the condition δ
+ ≥ 2 is essentially necessary to avoid (possibly many) trivial stationary measures (see Remark 1.4). Under the additional condition δ
− ≥ 2, Caputo and Quattropani [11]
showed that the random walk is ergodic with high probability (whp), so we call it the *ergodicity* regime, and that there exists C ≥ 1 such that, whp

$$\begin{array}{l}{{C^{-1}\frac{\log^{1-\gamma_{0}}n}{n}\leq\pi_{\mathrm{min}}\leq C\frac{\log^{1-\gamma_{1}}n}{n}\;,}}\\ {{C^{-1}\frac{\log^{1-\kappa_{1}}n}{n}\leq\pi_{\mathrm{max}}\leq C\frac{\log^{1-\kappa_{0}}n}{n}\;,}}\end{array}$$
$$\left(1.1\right)$$  $$\left(1.2\right)$$

where γ0 ≥ γ1 ≥ 1 are defined in terms of δ
− and ∆+, and κ0 ≤ κ1 ≤ 1 are defined in terms of δ
+
and ∆−. While these constants are different in general, γ0 = γ1 and κ0 = κ1 if there are a linearly many vertices with degrees (δ
−, ∆+) and (δ
+, ∆−), respectively. Finally, using the bound on πmin, the authors showed that in the ergodicity regime the cover time satisfies whp

$$C^{-1}n\log^{\gamma_{1}}n\leq\tau_{\mathrm{cov}}\leq C n\log^{\gamma_{0}}n\ .$$
−1n logγ1 n ≤ τ cov ≤ Cn logγ0 n . (1.3)
The main purpose of this paper is to study the extremal values of the stationary distribution outside the ergodicity regime. If δ
+ ≥ 2 but no condition on the minimum in-degree is imposed, the random walk might fail to be ergodic, but the stationary measure is whp unique; we call it the unicity regime. While (1.1) and (1.2) indicate that the extremal values exhibit logarithmic fluctuations in the ergodicity regime, our main result shows in that unicity regime πmin has polynomial deviations with respect to the typical stationary values. As an easy consequence of our proof, we determine the maximal hitting and the cover time up to subpolynomial multiplicative terms.

$$(1.3)$$

## 1.2 Notations And Results

Before stating our results, we need to define some parameters of the bi-degree sequence ~dn. Although n does not appear in many of the notations, the reader should keep in mind that all the parameters defined here depend on n.

Let nk,` := |{i : (d
− i
, d+
i
) = (*k, `*)}| be the number of (*k, `*) in ~dn. Let ∆± = maxi∈[n]{d
± i
}, δ± =
mini∈[n]{d
± i
}. Let D = (D−, D+) be the degrees (number of heads and tails) of a uniform random vertex. In other words, P {D = (k, `)} = nk,`/n.

The bivariate generating function of D is defined by

$$f(z,w):=\sum_{k,\ell\geq0}\mathbb{P}\left\{D=(k,\ell)\right\}z^{k}w^{\ell}\;.$$
$$(1.4)$$
$$(1.5)$$

Let λ := m/n. Consider a branching process with offspring distribution that has generating function 1 λ
∂f
∂w (z, 1). Let s
− be its survival probability and let ν :=
1 λ
∂f
∂w (1, 1) be its expected number of offspring, which we call the *expansion rate*. Define the *subcritical in-expansion rate* by

$$\hat{\nu}^{-}:=\frac{1}{\lambda}\frac{\partial^{2}f}{\partial z\partial w}(1-s^{-},1)\in[0,1)\;.$$
−, 1) ∈ [0, 1) . (1.5)
Define Dout = (D
$$\mathrm{\underline{\mathrm{\underline{\boldmath~\underline{\mathrm{\boldmath~\underline{\mathrm{\boldmath~\underline{\mathrm{\boldmath~}}}}}}}}}}D_{\mathrm{out}}=(D_{\mathrm{out}}^-,D_{\mathrm{out}}^+),$$
out), the *out-size-biased* distribution of D, by
$$\mathbb{P}\left\{D_{\mathrm{out}}=(k,\ell)\right\}:=\frac{\ell}{\lambda}\mathbb{P}\left\{D=(k,\ell)\right\}\;,\qquad\mathrm{for}\;k\geq0,\ell\geq0\;.$$
In words, Dout is the degree distribution of a vertex incident to a uniform random tail. We define Din analogously.

Let D˜out = (D˜ −
out, D˜ +
out) be the random vector with distribution

$$\mathbb{P}\left\{\bar{D}_{\mathrm{out}}=\left(k,\ell\right)\right\}\coloneqq\frac{k\left(1-s^{-}\right)^{k-1}}{\ell^{-}}\mathbb{P}\left\{D_{\mathrm{out}}=\left(k,\ell\right)\right\}\;,\qquad\mathrm{for}\;k\geq0,\ell\geq0\;.$$
$$(1.6)$$
$$(1.7)$$
$$(1.8)$$

Define the *subcritical in-entropy* by

$$\hat{H}^{-}:=\mathbb{E}\left[\log\hat{D}_{\mathrm{out}}^{+}\right]=\frac{1}{\hat{\nu}^{-}m}\sum_{i\in[n]}d_{i}^{-}\,d_{i}^{+}\,(1-s^{-})^{d_{i}^{+}-1}\log d_{i}^{+}\ .$$

This parameter can be seen as an average row entropy of certain transition matrix (see [6]) and is related to the typical weight of trajectories under subcritical in-growth.

The *large deviation rate function* (or *Cram´er function*) of Z = log D˜ +
out is defined by

$$I(z):=\operatorname*{sup}_{x\in\mathbb{R}}\{x z-\log\mathbb{E}[e^{x Z}]\}\ ,\qquad{\mathrm{for~}}z\in\mathbb{R}\ .$$
xZ]} , for z ∈ R . (1.9)
Note that I(Hˆ −) = 0 and I(z) = ∞ if z > log ∆+ or z < log δ
+. Let

$$\phi(a)\coloneqq\frac{1}{a}\left(|\log\hat{\nu}^{-}|+I(a\hat{H}^{-})\right)\,\tag{1.10}$$

and let a0 be its minimising value in [0, ∞), which is attained in [1, ∞) by the properties of I(z).

$$(1.9)$$

Conditioned on G~n, a simple random walk on G~n is a Markov process (Zt)t≥0 with state space
[n]. Given the current vertex Zt, the walk chooses an out-neighbour of Zt uniformly at random as Zt+1, which is always possible as we assume δ
+ ≥ 2. If as t → ∞ the distribution of Zt converges to the same distribution π regardless of the choice of Z0, i.e., if there exists a probability density function π on [n] such that

$$\operatorname*{lim}_{t\rightarrow\infty}\operatorname*{sup}_{i,j\in[n]}|\mathbb{P}\left\{Z_{t}=j\mid Z_{0}=i\right\}-\pi(j)|=0\;,$$
$$(1.11)$$
$$(1.12)$$
$\left(1.13\right)^{2}$
we say (Zt)t≥0 has *unique stationary distribution* π. Let

$$\pi_{\mathrm{min}}=\operatorname*{min}\left\{\pi(i):i\in[n],\pi(i)>0\right\}\;,\qquad\pi_{\mathrm{max}}=\operatorname*{max}\left\{\pi(i):i\in[n]\right\}\;.$$

Our main result is the following:
Theorem 1.1. *Assume that* δ
+ ≥ 2 and ∆± ≤ M where M ∈ N *is a fixed integer. With high*

probability,
$$\pi_{\mathrm{min}}=n^{-(1+\hat{H}^{-}/\phi(a_{0})+o(1))}\ .$$
−(1+Hˆ −/φ(a0)+o(1)). (1.13)
If δ
− ≥ 2, πmin satisfies (1.1). In this case, Theorem 1.1 implies the weaker statement πmin =
n
−1+o(1) as we have Hˆ − ≤ log M and φ(a) = ∞ for any a ≥ 0, since ˆν
− = 0. Thus, our main contribution is to show that when δ
− ∈ {0, 1}, the polynomial exponent of πmin is not −1 any more.

The additional exponent Hˆ −/φ(α0) comes from the fact that whp some vertices are exceedingly difficult to reach by a simple random walk. In fact, determining the minimal stationary value can be seen as a competitive combination of two factors: (1) being far from the bulk of other vertices, which is controlled by the term | log ˆν
−| in (1.10) and (2) having large branching factors in the trajectories leading to a vertex, which is controlled by the term I(aHˆ−) in (1.10). The optimal ratio is given by a0. In particular, the vertex that minimises the stationary value is at distance 1 a0φ(a0) +1 log ν + o(1)log n from the bulk of other vertices. Only when a0 = 1, this vertex coincides with the vertex that is furthest from the bulk, which is at distance 1 | log ˆν−| +1 log ν + o(1)log n
(see [9]). Remark 1.2. Formally, (1.13) should be read as

$$\frac{\log\pi_{\mathrm{min}}^{-1}}{\log n}\to1+\frac{\hat{H}^{-}}{\phi(a_{0})}\quad\mathrm{~in~probability,}$$

where we let πmin take an arbitrary value in case that the stationary distribution is non-unique.

From our proof, one can obtain an upper bound on the speed of convergence of order (log n)
−1/2.

Whp bounds for πmin that are tight up to a constant, like the ones in (1.1), are unlikely to hold in this setting due to the use of large deviation theory. In fact, we believe that the rate of convergence (log n)
−1/2cannot be vastly improved, as this is the rate of convergence in Cramer's theorem (see Theorem 2.4 in Section 2.3).

Remark 1.3. Let G~ sn be G~n conditioned on being a simple directed graph. Then G~ sn is distributed uniformly among all simple directed graphs with degree sequence ~dn. It is well-known that the probability of G~n being simple is bounded away from zero when the maximum degrees are bounded
(see, e.g., [4, 18]). Thus, Theorem 1.1 also holds for G~ sn
.

$$(1.14)$$

Remark 1.4 (Uniqueness of π). Theorem 1.1 requires δ
+ ≥ 2. If δ
+ = 0, then the stationary distribution is almost surely either trivial or non-unique. If δ
+ = 1 and P {D+ = 1} is bounded away from 0 as n → ∞, then with constant probability there will be multiple loops in G~n giving rise to multiple trivial stationary distributions. Proposition 4.1 shows that under δ
+ ≥ 2, (Zt)t≥0 has a unique stationary distribution whp. Unicity of the equilibrium measure whp can be also shown if δ
+ = 1 and P {D+ = 1} = o(1). It is likely that the conclusion of Theorem 1.1 still holds in this situation.

Remark 1.5 (Maximum stationary value). In this paper we turned our attention to πmin. By averaging, πmax ≥ 1/n. Moreover, one can check that the proof of the second inequality in (1.2)
(see [11, Section 3.5]) does not use any condition on δ
−. Therefore, πmax = n
−1+o(1) holds in the setting of Theorem 1.1. It would be interesting to understand the behaviour of πmax when the maximum in-degree goes to infinity as n → ∞.

Remark 1.6 (Explicit polynomial exponents for πmin). Since for a general distribution there is no closed-form expression for I(z), Theorem 1.1 provides an implicit polynomial exponent. Nevertheless, I(z) can be computed explicitly for some particular bi-degree sequences, yielding explicit polynomial exponents. In Subsection 5.2, we give two such examples: one where I(z) = ∞ for any z 6= Hˆ − and the other where I(z) is the large deviation rate function of the Bernoulli distribution with an affine transformation. If explicit bounds are required, there is an extensive literature on concentration inequalities for the sum of independent bounded random variables, such as Bernstein's and Bennett's inequalities, (see, e.g., [22]) which provide explicit lower bounds on I(z) in terms of the moments of the distribution. Alternatively, rigorous numeric bounds can be computed with interval arithmetic libraries such as [24]. Remark 1.7. We can choose D to make the polynomial exponent in (1.13) arbitrarily small. For instance, fixing M ∈ N and letting

$$\mathbb{P}\left\{D^{-}=1\right\}=\frac{M-2}{M-1}\,\quad\mathbb{P}\left\{D^{-}=M\right\}=\frac{1}{M-1}\,\quad\mathbb{P}\left\{D^{+}=2\right\}=1\,\tag{1.15}$$  we have $\hat{\nu}^{-}=1+O(M^{-1})$, $\hat{H}^{-}=\log2$ and $I(a\hat{H}^{-})=\infty$ for $a\neq1$. So $a_{0}=1$ and $\phi(a_{0})=\log\hat{\nu}^{-}|$.  
Thus, Hˆ −/φ(a0) = Ω(M) and we can make it as large as we want by increasing M.

Remark 1.8. Consider the empirical measure ψ =
1 n Pi∈[n]
δ{nπ(i)}; that is, n times the stationary value of a uniform random vertex. In [5], it was shown that there exists a deterministic law L
such that dW(ψ,L) → 0 in probability, where dW is the 1-Wasserstein metric related to optimal transport problems. Our proof of Theorem 1.1 allows us to control the lower tail of ψ: for every α ∈ [0, Hˆ −/φ(a0)] and letting β =
αφ(a0)
Hˆ − ∈ [0, 1], we have

$$\mathbb{E}[\psi((0,n^{-\alpha}])]=\frac{1}{n}\sum_{i\in[n]}\mathbb{P}\left\{0<\pi(i)\leq n^{-(1+\alpha)}\right\}=n^{-\beta+o(1)}\;.\tag{1.16}$$

By computing the second moment, it can be shown that ψ((0, n−α]) is concentrated around it expected value. See Subsection 4.6 for a discussion of the proof of (1.16).

Let G be a directed graph with vertex set [n] having an attractive scc C0 with vertex set V0.

Let (Zt)t≥0 be a simple random walk on G. Let τx(y) := inf{t ≥ 0 : Zt = *y, Z*0 = x}. The *maximal*

hitting time is defined as
$$\tau_{\mathrm{hit}}:=\operatorname*{max}_{\begin{array}{l}{x\in[n]}\\ {y\in{\mathcal{V}}_{0}}\end{array}}\mathbb{E}[\tau_{x}(y)]\ .$$
E[τx(y)] . (1.17)
Let $\tau_{x}^{C}:=\inf\{t\geq0:\mathcal{V}_{0}\subseteq\cup_{r=0}^{t}\{Z_{r}\},Z_{0}=x\}$. The _cover time_ is defined as 
$$(1.17)$$
$$\tau_{\mathrm{cov}}:=\operatorname*{max}_{x\in[n]}\mathbb{E}[\tau_{x}^{C}]\ .$$
$$(1.18)$$
$\uparrow$ . 
$$(1.19)$$
] . (1.18)
As a consequence of the proof of Theorem 1.1, we determine the maximal hitting and the cover time, up to subpolynomial terms. Theorem 1.9. Under the hypothesis of *Theorem 1.1, whp*

$$\tau_{\mathrm{hit}}=n^{1+\hat{H}^{-}/\phi(a_{0})+o(1)},$$
1+Hˆ −/φ(a0)+o(1), τ cov = n
$$\tau_{\mathrm{cov}}=n^{1+\hat{H}^{-}/\phi(a_{0})+o(1)}\ .$$

To simplify the notations, we avoid using d·e and b·c to make certain parameters integers. Such omissions should be clear from the context and do not affect the validity of the proofs.

The rest of paper contains four sections: Section 2 studies the properties of marked branching processes; Section 3 describes a graph exploration process and shows that it can be coupled with a marked branching process; using these results, Section 4 proves Theorem 1.1; finally in Section 5 we give some applications, including the proof of Theorem 1.9.

## 2 Marked Branching Processes

In this section, we prove some general results for the marked branching process defined below. This part of the paper may be of independent interest.

## 2.1 Marked Branching Processes

Let η = (*ξ, ζ*) be a random vector on Z
2≥0 and let (ηi,t = (ξi,t, ζi,t))i≥1,t≥0 be iid (independent and identically distributed) copies of η. The *branching process*, also known as the *Galton-Watson tree*,
(Xt)t≥0 with offspring distribution ξ is defined by

$$X_{t}=\begin{cases}1&\text{if}t=0\;,\\ \sum_{i=1}^{X_{t-1}}\xi_{i,t-1}&\text{if}t\geq1\;.\end{cases}$$
$$(2.1)$$

We call Xt the t-th *generation* and refer to (Xr)t≥r≥0 as the first t *generations*. For an individual
(*i, t*), we call i its *sibling index* and t its *generation index*. If the individual (*i, t*) is marked (labelled)
by the integer ζi,t, then we call (Xt)t≥0 the *marked branching process* with offspring distribution η.

Let g be the bivariate probability generating function of η, i.e.,

$$g(z,w):=\sum_{k,\ell\geq0}\mathbb{P}\left\{\eta=(k,\ell)\right\}z^{k}w^{\ell}\;.$$

and let g(z) := g(z, 1) be the probability generating function of ξ. Define ν := E [ξ] = g 0(1) . (2.3)

$$(2.2)$$

Given i ∈ [Xt] := {1*, . . . , X*t} and r ∈ [0, t], let f r(*i, t*) ∈ [Xt−r] be the sibling index of (*i, t*)'s ancestor r generations away. We write f(*i, t*) = f 1(*i, t*) and note f 0(*i, t*) = i. Let

$$\Gamma_{i,t}:=\prod_{r=1}^{t}\frac{1}{\zeta_{f^{t-r}(i,t),r}}\;,\qquad\Gamma_{t}:=\sum_{i=1}^{X_{t}}\Gamma_{i,t}\;.\tag{2.4}$$

In this section, we will mostly be interested in the sequence of random variables (Γt)t≥1. Let Ft be the σ-algebra generated by (ξi,r)i≥1,t>r≥0,(ζi,r)i≥1,t≥r≥0. Note that Xt and Γt are measurable with respect to Ft. Moreover, Γt > 0 if and only if Xt > 0.

The following lemma is similar to [10, Lemma 17].

Lemma 2.1. If E[ξ/ζ] ∈ (0, ∞)*, then* ΓtE [ξ/ζ]
−t*is a martingale with respect to* (Ft)t≥0.

Proof. We have

$$\mathbb{E}[\Gamma_{t}\mid\mathcal{F}_{t-1}]=\sum_{j=1}^{X_{t-1}}\mathbb{E}\left[\sum_{i:f(i,t)=j}\Gamma_{i,t}\left|\ \mathcal{F}_{t-1}\right.\right]$$ $$=\sum_{j=1}^{X_{t-1}}\Gamma_{j,t-1}\mathbb{E}\left[\sum_{i:f(i,t)=j}1/\zeta_{i,t}\left|\ \mathcal{F}_{t-1}\right.\right]$$ $$=\sum_{j=1}^{X_{t-1}}\Gamma_{j,t-1}\mathbb{E}\left[\zeta_{j,t-1}\mid\mathcal{F}_{t-1}\right]\mathbb{E}\left[1/\zeta_{i,t}\mid\mathcal{F}_{t-1}\right]$$ $$=\sum_{j=1}^{X_{t-1}}\Gamma_{j,t-1}\mathbb{E}[\zeta/\zeta]=\mathbb{E}[\zeta/\zeta]\Gamma_{t-1}\,$$

where we use Wald's equation and that {ξj,t−1} ∪ {ζi,t}i: f(i)=jis a mutually independent collection of random variables, conditional on Ft−1.

Remark 2.2. Recall the definition of the distribution Dout in (1.6) and let η L= Dout. Since ` ≥
δ
+ ≥ 2, we have

$$\mathbb{E}[\xi/\zeta]=\mathbb{E}[D^{-}_{\rm out}/D^{+}_{\rm out}]=\sum_{k,\ell\geq1}\frac{k}{\ell}\cdot\frac{\ell n_{k,\ell}}{m}=\frac{1}{m}\sum_{k,\ell\geq1}kn_{k,\ell}=1.\tag{2.5}$$

In other words, if we take η = Dout, Γtis a martingale by Lemma 2.1. We have formulated the lemma in a way that will allow us to deal with small perturbations of Dout.

## 2.2 Conditioned Branching Processes

Before introducing the results, some more definitions are needed.

## 2.2.1 Conditioned On Extinction

Let s := P {∩t≥0[Xt > 0]} be the *survival probability* of (Xt)t≥0. The *conjugate probability distribution* of ξ, denoted by ˆξ, is defined as

$$\mathbb{P}\left\{\hat{\xi}=k\right\}:=(1-s)^{k-1}\mathbb{P}\left\{\xi=k\right\}\,,$$
$$(2.6)$$

when s < 1, while

$$\mathbb{P}\left\{{\hat{\xi}}=1\right\}=\mathbb{P}\left\{\xi=1\right\}\ ,\qquad\mathbb{P}\left\{{\hat{\xi}}=0\right\}=1-\mathbb{P}\left\{\xi=1\right\}\ ,$$

when s = 1. Define the *subcritical expansion rate* as

$$(2.7)$$
$$(2.8)$$
$$\hat{\nu}:=\mathbb{E}[\hat{\xi}]=g^{\prime}(1-s)\in[0,1)\ .$$

0(1 − s) ∈ [0, 1) . (2.8)
If η L= Dout, then ˆν = ˆν
− where ˆν
− is defined in (1.5).

The following *duality* is well-known:
Theorem 2.3 (see, e.g., Theorem 3.7 in [25]). Let (Xt)t≥0 be a branching process with offspring distribution ξ and survival probability s. If s < 1, then the branching process (Xt)t≥0 *conditioned* on extinction is distributed as a branching process with offspring distribution ˆξ.

## 2.2.2 Conditioned On Survival

Let (X∗
t
)t≥0 ⊆ (Xt)t≥0 be the subprocess of the individuals that have some surviving progeny.

Thus, P {X∗
0 = 0} = 1 − s and P {X∗
0 = 1} = s. Conditioning on [X∗
0 = 1], i.e., survival of (Xt)t≥0,
(X∗
t
)t≥0 is a branching process with offspring distribution ξ
∗, defined by

$$\mathbb{P}\left\{\xi^{*}=k\right\}={\frac{\sum_{m\geq k}\mathbb{P}\left\{\xi=m\right\}\binom{m}{k}s^{k}(1-s)^{m-k}}{s}}={\frac{s^{k-1}g^{(k)}(1-s)}{k!}}\ ,\qquad{\mathrm{for~}}k\geq1\ .$$
A simple computation gives
$$\mathbb{E}\left[\xi^{*}\right]=\sum_{k=1}^{\infty}\frac{k s^{k-1}g^{(k)}(1-s)}{k!}=\sum_{k=0}^{\infty}\frac{s^{k}g^{(k+1)}(1-s)}{k!}=g^{\prime}(1)=\nu\;.\tag{1}$$
$$(2.9)$$
$$(2.10)$$

Moreover,
$$\mathbb{P}\left\{\xi^{*}=1\right\}=\mathbb{P}\left\{X_{1}^{*}=1\mid X_{0}^{*}=1\right\}=g^{\prime}(1-s)=\hat{\nu}\ .$$
0(1 − s) = ˆν . (2.11)
Let (X˜t)t≥0 ⊆ (X∗
t
)t≥0 be the subprocess of the individuals that have *exactly one* surviving element in their offspring. (Note that (X˜t)t≥0 is not necessarily a connected process.) Then conditioned on (i, t) ∈ (X˜t)t≥0, ξi,t is distributed as ˜ξ, defined by

$$\mathbb{P}\left\{{\hat{\xi}}=k\right\}=\mathbb{P}\left\{X_{1}=k\mid X_{1}^{*}=1\right\}={\frac{k(1-s)^{k-1}}{{\hat{\nu}}}}\mathbb{P}\left\{\xi=k\right\},\quad{\mathrm{for~}}k\geq1\ .$$
$$(2.12)$$

Let ˜η = (˜ξ, ˜ζ) be the distribution of ηi,t conditioned on (*i, t*) ∈ (X˜t)t≥0. Define the subcritical entropy of η as

$$\tilde{H}:=\mathbb{E}[\log\tilde{\zeta}]=\frac{1}{\tilde{\nu}}\sum_{k\geq1,\ell\geq0}k(\log\ell)(1-s)^{k-1}\mathbb{P}\left\{\eta=(k,\ell)\right\}=\frac{\mathbb{E}[\hat{\xi}\log\zeta]}{\mathbb{E}[\hat{\xi}]}.\tag{2.13}$$

8 This parameter is central in our results. If η L= Dout, then Hˆ = Hˆ − where Hˆ − is defined in (1.8).

Later, we will also consider the *inhomogeneous* branching process (Xˆt)t≥0 in which the root has offspring distribution ˜ξ − 1 and all other individuals have offspring distribution ˆξ. Note that such a process will almost surely become extinct.

## 2.3 Large Deviation Theory

We will use Cram´er's theorem, a classical result in large deviation theory.

Theorem 2.4 (see, e.g., Corollary 2.2.19 in [16]). Let Z1, . . . , Zt *be iid copies of a random variable* Z *satisfying* E[e λZ] < ∞ for all λ ∈ R*. Define*

$$\bar{Z}_{t}=\frac{1}{t}\sum_{i=1}^{t}Z_{i}\ .$$
$$(2.14)$$
$$(2.15)$$
$$(2.16)$$
$$Then,\;f o r\;a n y\;z\geq\mathbb{E}[Z]$$
$$\operatorname*{lim}_{t\rightarrow\infty}\frac{1}{t}\log\mathbb{P}\left\{\bar{Z}_{t}\geq z\right\}=-I(z)\ ,$$
$$u h e r e$$
$$I(z):=\operatorname*{sup}_{\lambda\in\mathbb{R}}\{z\lambda-\log\mathbb{E}[e^{\lambda Z}]\}\ ,\qquad f o r\ z\in\mathbb{R}\ ,$$

is the Fenchel-Legendre transform *of the cumulant generating function of* Z.

From now on we will take Z to be the discrete, random variable log ˜ζ with support a subset of
{log 2, log 3*, . . . ,* log M}, where M is a fixed integer, and expected value Hˆ . Thus, I(z) will refer to the large deviation rate function of log ˜ζ which has the properties that on z ∈ [H, ˆ log M), I(z) is continuous, non-decreasing, with I
0(z) ∈ [0, ∞) and I(Hˆ ) = 0. Moreover, I(z) is non-increasing on
(−∞, Hˆ ). The proof for these properties follow along the line of Lemma 2.2.5 in [16].

## 2.4 Subcritical Growth: A Lower Bound

Theorem 2.5. Let (Xr)r≥0 *be a marked branching process with offspring distribution* η = (ξ, ζ) *with* E[ξ] ∈ (1, ∞). Suppose that ξ ≤ M and 2 ≤ ζ ≤ M *almost surely. Then for any* a ∈ [1, log(M)/Hˆ ],
t → ∞ and ω ≥ t,

$$\mathbb{P}\left\{\left[0<\Gamma_{t}<e^{-a\hat{H}t}\right]\cap\bigcap_{r=1}^{t}[0<X_{r}<\omega]\right\}\geq\exp\left\{-\left(\left|\log\hat{\nu}\right|+I(a\hat{H})+o(1)\right)t\right\}.\tag{2.17}$$

The important event in the previous theorem is the first one, regarding Γt. The event controlling the size of the first t generations is only added to allow coupling the branching process with the graph exploration later in the paper.

Proof. For the sake of simplicity, we first prove the theorem assuming that 1 is in the support of ξ. The modifications needed otherwise, are detailed at the end of the proof.

Consider the events E1 = [X∗
t = 1], E2 = [Xt = 1] and E3 =Ttr=1[0 < Xr < ω]. The idea of the proof is to lower bound the probability in (2.17) conditioning on these events.

When the event E1 happens, we call the first t generations of (X∗
r
)r≥0 the *spine*. We may assume without loss of generality that the spine of survival individuals corresponds to the first individual in each generation, since reordering sibling indices does not change the value of Γt or Xr. Moreover, the number of children (in (Xr)r≥0) and the mark of each individual in the spine is jointly distributed as ˜η = (˜ξ, ˜ζ).

For x = (x1, . . . , xt) ∈ M := {1} × [M]
t−1, let F(x) be the intersection of the event E1 and the event that ξ1,0 = xt*, . . . , ξ*1,t−1 = x1, i.e., the r-generation of the spine has xt−r children. (We require x1 = 1, which is in the support of ξ, so that F(x) ∩ E2 is not empty.) On the event F(x),
the first t generations of the Galton-Watson tree (Xr)r≥0 can be constructed equivalently as follows
- First start with a one-ary tree (a path) of length t which serves as the spine. Then for the r-generation individual in the spine with r ∈ {0*, . . . , t* − 1}, attach an independent copy of the branching process (Xˆj )j≥0 (defined in Subsection 2.2.2), conditioned on its root having xt−r − 1 children.

On E1, let us write

$$\Gamma_{t}=\Gamma_{1,t}+\sum_{i=2}^{X_{t}}\Gamma_{i,t}=:\Gamma_{t}^{*}+\Gamma_{t}^{0}\ .$$
$$(2.18)$$

As E2 implies [Γ0 t = 0], the desired probability is at least

$$\mathbb{P}\left\{[0<\Gamma_{t}^{*}<e^{-a\hat{H}t}]\cap E_{1}\cap E_{2}\cap E_{3}\right\}\geq\mathbb{P}\left\{E_{1}\right\}\mathbb{P}\left\{\Gamma_{t}^{*}<e^{-a\hat{H}t}\mid E_{1}\right\}\min_{\mathbf{x}\in\mathcal{M}}\mathbb{P}\left\{E_{2}\cap E_{3}\mid F(\mathbf{x})\right\}\,,\tag{2.1}$$
(2.19)
where we use that given F(x), E2 ∩ E3 is independent from [Γ∗
t < e−aHt ˆ].
Let us first bound the probability of E1. By (2.11), one has
$$\mathbb{P}\left\{E_{1}\right\}=\mathbb{P}\left\{X_{0}^{*}=1\right\}\prod_{i=1}^{t}\mathbb{P}\left\{X_{i}^{*}=1\mid X_{i-1}^{*}=1\right\}=\mathbb{P}\left\{X_{0}^{*}=1\right\}\mathbb{P}\left\{\xi^{*}=1\right\}^{t}=s\nu^{t}\;.$$
t. (2.20)
We now bound Γ∗
t on E1. Let ζr be the mark of the spine individual in generation r. Then ζr are iid copies of ˜ζ. Letting Zr = log ζr we have

$$\Gamma_{t}^{*}=\prod_{r=1}^{t}(\zeta_{r})^{-1}=e^{-\sum_{r=1}^{t}Z_{r}}\;.$$
$$(2.20)$$

As t → ∞, it follows from Cram´er's theorem (Theorem 2.4) that

$$\mathbb{P}\left\{\Gamma_{t}^{*}<e^{-a{\hat{H}}t}\;\Big\vert\;E_{1}\right\}=e^{-(1+o(1))I(a{\hat{H}})t}\;.$$
−(1+o(1))I(aHˆ )t. (2.22)
We finally obtain a bound on the probability of E2 ∩ E3 conditional on F(x), uniform over x ∈ M. By the independence of the trees attached to the spine,

$$\mathbb{P}\left\{E_{2}\mid F(\mathbf{x})\right\}=\prod_{r=0}^{t-1}\mathbb{P}\left\{\hat{X}_{t-r}=0\ \Big{|}\ \hat{X}_{1}=x_{t-r}-1\right\}=\prod_{r=2}^{t}\mathbb{P}\left\{\hat{X}_{r}=0\ \Big{|}\ \hat{X}_{1}=x_{r}-1\right\}\,\tag{2.23}$$
$$(2.21)$$
$$(2.22)$$

where the last step uses that x1 = 1. For r ≥ 2, using that xr ≤ M, we have

$$\mathbb{P}\left\{\hat{X}_{r}=0\ \Big{|}\ \hat{X}_{1}=x_{r}-1\right\}\geq\mathbb{P}\left\{\hat{X}_{2}=0\ \Big{|}\ \hat{X}_{1}=M\right\}=\mathbb{P}\left\{\hat{\xi}=0\right\}^{M}.\tag{2.24}$$

10 Also, by Markov inequality, there exists a constant r0 such that for all r ≥ r0

$$\mathbb{P}\left\{{\hat{X}}_{r}\geq1\;{\Bigm|}{\hat{X}}_{1}=x_{r}\right\}\leq\mathbb{E}\left[{\hat{X}}_{r}\;{\Bigm|}{\hat{X}}_{1}=M\right]=M{\hat{\nu}}^{r-1}\leq1/2\;.$$
r−1 ≤ 1/2 . (2.25)
It follows that

$$(2.25)$$
$$\mathbb{P}\left\{E_{2}\mid F(\mathbf{x})\right\}\geq\mathbb{P}\left\{\hat{\xi}=0\right\}^{r_{0}M}\prod_{r>r_{0}}\left(1-M\hat{\nu}^{r-1}\right)>c_{0}\;,$$  3. 
$$(2.26)$$

for some constant c0 > 0.

To bound the probability of E3, we use the same argument as in [9, Theorem 3.4]. Note that Xr > 0 is already implied by E1, it suffices to bound the probability Xr is not too large. By linearity of expectation,

$$\mathbb{E}[X_{r}\mid F({\bf x})]=1+\sum_{j=1}^{r}(x_{j-r+t}-1)\hat{\nu}^{j-1}=O(1)\;,$$

and by independence of the branching subtrees

$$\mathrm{Var}\left(X_{r}\mid F(\mathbf{x})\right)\leq\sum_{j=1}^{r}(x_{j-r+t}-1){\frac{\mathrm{Var}({\hat{\xi}})\hat{\nu}^{j-2}\left(\hat{\nu}^{j-1}-1\right)}{\hat{\nu}-1}}=O(\mathrm{Var}({\hat{\xi}}))=O(1)\ ,$$

where we use the moment formula in [3, pp. 4]. Thus, we have E[Xˆ 2 r
] = O(1) and it follows from Chebyshev's inequality that

$$\mathbb{P}\left\{E_{3}^{c}\mid F(\mathbf{x})\right\}\leq\sum_{r=0}^{t}\mathbb{P}\left\{X_{r}\geq\omega\mid F(\mathbf{x})\right\}\leq\sum_{r=1}^{t}{\frac{\mathbb{E}\left[X_{r}^{2}\mid F(\mathbf{x})\right]}{\omega^{2}}}=O\big(t/\omega^{2}\big)=O(t^{-1})\ .$$
$$(2.27)$$
−1) . (2.27)
From (2.26) and (2.27), we obtain P {E2 ∩ E3 | F(x)} ≥ P {E2 | F(x)} − P {E
c 3 | F(x)} ≥ c0/2 . (2.28)
The desired bound follows from plugging (2.20), (2.22) and (2.28) into (2.19).

If the minimal positive support of ξ is k0 ≥ 2, then the only change needed is to let E2 = [Xt =
k0]. The extra k0 − 1 individuals in generation t contribute at most k0MΓ1,t to Γt. Thus the same argument still works.

## 2.5 Subcritical Growth: An Upper Bound

Given a fixed γ > 0, let Gt(γ) be the event ∩i∈[Xt][Γi,t ≥ γ]. In the rest of Subsection 2.5, we will prove the following theorem:
Theorem 2.6. Let (Xr)r≥0 *be a marked branching process with offspring distribution* η = (ξ, ζ) *with* E[ξ] ∈ (1, ∞). Suppose that ξ ≤ M and 2 ≤ ζ ≤ M almost surely. Let ω → ∞, t ∈ (log2 *ω, ω*1/2)
and a ≥ 1*. Then, we have*

$$\mathbb{P}\left\{(G_{t}(e^{-a\hat{H}t}))^{c}\cap[0<X_{t}<\omega]\right\}\leq\exp\left\{-\left(|\log\hat{\nu}|+I(a\hat{H})+o(1)\right)t\right\}\;.\tag{2.29}$$

## 2.5.1 An Inhomogeneous Branching Process

Fix t and let (X
(t)
r )t≥r≥0 ⊆ (Xr)r≥0 be the finite subprocess containing individuals in the first t generations that have progeny in generation t. Similar to X∗
r
, X
(t)
r is non-decreasing in r Conditioned on the event [Xt > 0], (X
(t)
r )t≥r≥0 can be seen as an inhomogeneous branching process. The offspring distribution of the individuals in generation r = t − a in this process is ξ
(a), defined by

$$\mathbb{P}\left\{\xi^{(a)}=k\right\}=\frac{1}{s_{a}}\sum_{m\geq k}\mathbb{P}\left\{\xi=m\right\}\binom{m}{k}s_{a-1}^{k}(1-s_{a-1})^{m-k}=\frac{(s_{a-1})^{k}}{s_{a}k!}h^{(k)}(1-s_{a-1})\;,\qquad\text{for}k\geq1\;,\tag{2.30}$$  where $s_{a}:=\mathbb{P}\left\{X_{a}>0\right\}$.  
$$(2.32)$$

$$(2.33)$$

Note the similarity between ξ
(a) and ξ
∗ which is defined in (2.9). We have sa = s + O(ˆν a)
(see [9, Eq. (3.6)]). Using the Taylor expansion of h
(k) around 1 − s, we get

$$\mathbb{P}\left\{\xi^{(a)}=k\right\}=\frac{s^{k-1}}{k!}h^{(k)}(1-s)+O(\hat{\nu}^{a})=\mathbb{P}\left\{\xi^{*}=k\right\}+O(\hat{\nu}^{a})\;,\qquad\text{for}0\leq a\leq t,\ k\geq1\;.\tag{2.31}$$

In particular, by (2.11)

$$\mathbb{P}\left\{\xi^{(a)}=1\right\}=\mathbb{P}\left\{\xi^{*}=1\right\}+O(\hat{\nu}^{a})=\hat{\nu}+O(\hat{\nu}^{a})\;.$$
a) . (2.32)
When 0 ≤ ξ ≤ M almost surely, it follows from (2.31) that

$$\mathbb{E}[\xi^{(a)}]=\mathbb{E}[\xi^{*}]+O(\hat{\nu}^{a})=\nu+O(\hat{\nu}^{a})\ .$$
a) . (2.33)

## 2.5.2 Control The Surviving Process

Denote by Pt {·} := P *{· |* Xt > 0} the probability conditioned to survival at time t.

The argument for the following lemma is similar that of Theorem 3.4 in our previous work [9].

We give a proof for completeness.

Lemma 2.7. Let t and ω be as in *Theorem 2.6. Set* t0 :=
1 | log ˆν| +1 log ν log ω = o(t)*. There exists* a constant C0 ≥ 1 *such that for any* h ≤ t − t0, i ≤ h,

$\lambda$ l.e. 
$$\mathbb{P}_{t}\left\{X_{i}^{(t-h+i)}<\omega\right\}\leq C_{0}\hat{\nu}^{i-t_{0}}\ .$$
$$(2.34)$$

i−t0. (2.34)
Proof. Conditioned on survival at time t − r, (X
(t−r)
j)t−r≥j≥0 is a branching process where the individuals at generation j have offspring distribution ξ
(t−r−j) defined in (2.30). Recall that E[ξ
∗] =
ν > 1. By (2.33) and since r + j ≤ h ≤ t − t0, by the choice of t0, we have

$$\mathbb{P}\left\{\xi^{(t-r-j)}=k\right\}=\mathbb{P}\left\{\xi^{*}=k\right\}+O(\dot{\nu}^{t_{0}})=\mathbb{P}\left\{\xi^{*}=k\right\}+O(\omega^{-1})\ ,\qquad{\mathrm{for~}}k\geq1\ ,$$
and similarly
$\mathbb{E}\left[\xi^{(t-r-j)}\right]=\nu(1+O(\omega^{-1}))$.  $\circ\geq\omega^{-1}$: this is possible as $\hat{\nu}<1$. Let $\xi^{-}$ be a fixed dist:
Choose ε > 0 so ((1 − ε)ν)
t0 ≥ ω
− be a fixed distribution such that each ξ
(t−r−j)stochastically dominates ξ
− and ν
− := E[ξ
−] ≥ ν(1 − ε). Let (X
−
j
)j≥0 be a

$$(2.35)$$
$$(2.36)$$

branching process with offspring distribution ξ
−. The processes (X
(t−r)
j)h−r≥j≥0 and (X
−
j
)j≥0 can be coupled so X
(t−r)
j ≥ X
−
jalmost surely for every j ≤ h − r.

By a theorem due to Kesten and Stigum [9, Theorem 3.1], conditioned on survival, (ν
−)
−jX
−
j converges almost surely to a non-degenerated random variable W which is absolutely continuous on (0, ∞). Writing i = h − r, let ai:= Pt−h+i{X
(t−h+i)
i < ω}. It follows that

$$1-a_{t_{0}}=\mathbb{P}_{t-h+t_{0}}\big{(}X_{t_{0}}^{(t-h+t_{0})}\geq\omega\big{)}$$ $$\geq\mathbb{P}\left\{X_{t_{0}}^{\star}\geq\omega\right\}\geq\mathbb{P}\left\{X_{t_{0}}^{\star}\geq(\nu^{-})^{t_{0}}\right\}\geq\mathbb{P}\left\{1\leq(\nu^{-})^{-t_{0}}X_{t_{0}}^{\star}\leq2\right\}\to\mathbb{P}\left\{1\leq W\leq2\right\}>0.\tag{2.3}$$  As $t_{0}\to\infty$, $1-a_{t_{0}}$ is bounded away from $0$. So we have $a_{t_{0}}\leq1-c_{0}$, for some $c_{0}>0$.  
By splitting depending on whether the offspring of the first generation is one or larger, we have
the simple recursive inequality for *i > t*0:
$$\begin{array}{l}{{>0\ .}}\\ {{(2.37)}}\end{array}$$
$$\begin{array}{r l}{{a_{i}\leq\mathbb{P}\left\{\xi^{(t-h+i)}=1\right\}a_{i-1}+\left(1-\mathbb{P}\left\{\xi^{(t-h+i)}=1\right\}\right)a_{i-1}^{2}}}\\ {{}}&{{=\dot{\nu}a_{i-1}+(1-\dot{\nu})a_{i-1}^{2}+O(\dot{\nu}_{\xi}^{t-h+i}a_{i-1})\;.}}\end{array}$$
$$(2.38)$$

This recursion has exactly the same form of as [23, Eq. (2.4)] and can be solved the same way to show that there exists a constant C0 such that for i ≤ h

$$a_{i}=\mathbb{P}_{t}\left\{X_{i}^{(t-h+i)}<\omega\right\}\leq C_{0}\hat{\nu}^{i-t_{0}}\ .$$
$$(2.39)$$
$$\square$$
i−t0. (2.39)

## 2.5.3 Ramifications In The Spine

Conditioned on survival at time t, let x be an individual of generation h of (X
(t)
r )t≥r≥0. Let y0, y1*, . . . , y*h = x be the path connecting the root y0 to x, which we refer to as the spine associated to x. An index r ∈ {0*, . . . , h* − 1} is a *ramification*1 of the spine, if yr has offspring at least 2 in
(X
(t)
r )t≥r≥0. Let R(x) to be the number ramifications of the spine associated to x. The following result refines Lemma 2.7 by considering the number of ramifications.

Lemma 2.8. Let t0 be as in Lemma 2.7. There exists a constant C1 > 0 *such that, for* h ≤ t − t0 and every individual x *of generation* h of (X
(t)
r )t≥r≥0*, we have*

$$\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R(x)\geq\ell\right\}\leq\hat{\nu}^{h+(t_{0}-C_{1})(\ell-2t_{0})}\ ,\qquad f o r\ \ 2t_{0}<\ell\leq h\ .$$

Proof. By permuting the sibling index of individuals, we may assume that x is the first individual of X
(t)
h
. One can decompose the set of individuals in each generation of (X
(t)
r )t≥r≥0 according to their the last ancestor with x, i.e., the last of their ancestors that belongs to the spine y0*, . . . , y*h = x.

Conditioning on [Xt > 0], the number of children of y0*, . . . , y*h−1 in (X
(t)
r )t≥r≥0 is distributed as independent random variables ξ0*, . . . , ξ*h−1 where ξr L= ξ
(t−r).

Therefore, we can generate (X
(t)
r )h≥r≥0 equivalently as follows: (i) construct the spine; (ii) for every r ∈ [h−1] attach ξr −1 independent copies of (X
(t−(r+1))
j)h−r−1≥j≥0 conditioned on [Xt > 0],
which we denote by (W
r,2 j)h−(r+1)≥j≥0*, . . . ,*(W
r,ξr j)h−(r+1)≥j≥0, to yr.

1The word "ramification" means "a complex or unwelcome consequence of an action or event."

$$(2.40)$$

Looking at the generation h, this decomposition gives the following recursive inequality:

$$X_{h}^{(t)}=1+\sum_{r=0}^{h-1}\sum_{k\geq2}^{\xi_{r}}W_{-(r+1)}^{rk}\geq\sum_{i=1}^{h}Z_{i}\succeq\sum_{i=20+1}^{h}Z_{i}\,\tag{2.41}$$  where $\succeq$ denotes _stochastically domination_ and $Z_{1},\ldots,Z_{h}$ are independent random variables with 
distribution
$$Z_{i}\!\stackrel{\cal L}{=}\,\begin{cases}0&\mathrm{with~probability~}b_{i}\;,\\ \left(X_{i-1}^{(t-h+i-1)}\;\Big\vert\;X_{t-h+i-1}>0\right)&\mathrm{with~probability~}1-b_{i}\;,\end{cases}$$

where bi:= Pξ
(t−h+i) = 1	.

Let R0(x) be the number of ramifications of the spine associated to x with index at most h − 2t0 − 1. Let p`:= Pt nX
(t)
h *< ω, R*0(x) = `
o. Recalling that ai:= Pt−h+i{X
(t−h+i)
i < ω}, we have

$$p_{\ell}\leq\sum_{i_{1}<i_{2},\ldots<i_{\ell}}\prod_{j=1}^{\ell}a_{i_{j}-1}(1-b_{i_{j}})\prod_{\begin{subarray}{c}2t_{0}<j\leq T\\ j\notin\{i_{1},\ldots,i_{\ell}\}\end{subarray}}b_{j}\,\tag{2.42}$$  _has a free-valued additive identity $\ell$ for $\ell\in\{0,\ell,T\}$ and bi-dividing $\ell$ for $\ell\in\{0,\ell,T\}$._
where the sum is over all choices of ` ordered and distinct indices from {2t0 + 1, T}, which indicate where the ramifications occur. Since t − h ≥ t0 and by (2.32), we have that bi = ˆν + O(ˆν t−h+i) =
νˆ + O(ˆν 3t0 ) for all 2t0 ≤ i ≤ h. Thus, (2.42) implies that

$1\ 2t_{0}\leq i\leq h$. Thus, (2.42) implies that  $$p_{\ell}\leq(1-\hat{\nu}+O(\hat{\nu}^{2t_{0}}))^{\ell}(\hat{\nu}+O(\hat{\nu}^{3t_{0}}))^{h-2t_{0}-\ell}\sum_{i_{1}<i_{2}\cdots<i_{\ell}}\prod_{j=1}^{\ell}a_{i_{j}-1}$$ $$\ell$$
$$(2.43)$$
$$(2.44)$$
$$=(1+o(1))(1-\tilde{\nu})^{\ell}\hat{\nu}^{h-2t_{0}-\ell}\sum_{i_{1}<i_{2}\cdots<i_{\ell}\,j=1}\prod_{i=1}^{\ell}a_{i_{j}-1}\;,$$
where the last step uses that ˆν t0 ≤ ω
−1, that h ≤ t ≤
√ω and ω → ∞. Moreover,

$$\sum_{i_{1}<\cdots<i_{\ell}}\prod_{j=1}^{\ell}a_{i_{j}-1}\leq\left(\sum_{i=2i_{0}+1}^{h}a_{i-1}\right)^{\ell}\leq\left(\sum_{i=2i_{0}+1}^{\infty}C_{0}\hat{\nu}^{i-t_{0}-1}\right)^{\ell}\leq\left(C_{0}\frac{\hat{\nu}^{t_{0}}}{1-\hat{\nu}}\right)^{\ell}\;.$$

`. (2.44)
Putting this back to (2.43) we have

$$p_{\ell}=(1+o(1))\hat{\nu}^{h-2t_{0}}(C_{0}\hat{\nu}^{t_{0}-1})^{\ell}\;.$$
`. (2.45)
Since there are at most 2t0 − 1 ramifications in the last 2t0 − 1 indices of the spine (excluding x),
it follows that for ` > 2t0,

$$\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R(x)\geq\ell\right\}\leq\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R_{0}(x)\geq\ell-2t_{0}\right\}$$ $$=\sum_{j\geq\ell-2t_{0}}p_{j}\tag{2.46}$$ $$=O(1)\ell^{h-2t_{0}}\left(C_{0}\ell^{j\ell_{0}-1}\right)^{\ell-2t_{0}}$$ $$\leq\dot{p}^{h+(t_{0}-C_{1})(\ell-2t_{0})}\,$$
$$\mathbb{D}$$

for some constant C1 > 0.

14

## 2.5.4 Finishing The Proof Of Theorem 2.6

Let h = t − t0 and define
$$\ell(t):=2t_{0}+\frac{t+t_{0}}{t_{0}-C_{1}}>2t_{0}\;.$$
> 2t0 . (2.47)
Then, by Lemma 2.8

$$\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R(x)\geq\ell(t)\right\}\leq\hat{\nu}^{2t}\;.$$
2t. (2.48)
Clearly, the event $[X_{t}<\omega]$ implies $[X_{h}^{(t)}<\omega]$, for every $h\leq t$. Let $x_{1},\ldots,x_{X_{h}^{(t)}}$ denote the individual in expectation $h$ of $(X_{h}^{(t)})$. Let $F_{h}$ be the event $\mathcal{C}_{h}^{X_{h}^{(t)}}[B(x_{1})<\ell(t)]$. By a union 
individuals in generation h of (X
r )t≥r≥0. Let E1 be the event ∩
i=1 [R(xi) < `(t)]. By a union bound over the choice of i and using (2.48)

$$\mathbb{P}_{t}\left\{X_{t}<\omega,E_{1}^{c}\right\}\leq\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,E_{1}^{c}\right\}\leq\sum_{j=1}^{\omega-1}\sum_{i=1}^{j}\mathbb{P}_{t}\left\{X_{h}^{(t)}=j,R(x_{i})\geq\ell(t)\right\}\leq\omega\hat{\nu}^{2t}\.$$
$$(2.47)$$

$$(2.48)$$

2t. (2.49)
Let E2 := [0 < Xt < ω] ∩ E1. Since X
(t)
his non-decreasing, by Lemma 2.7 with i = h = t − t0, we
have
$$\mathbb{P}_{t}\left\{E_{2}\right\}\leq\mathbb{P}_{t}\left\{X_{t}<\omega\right\}\leq\mathbb{P}_{t}\left\{X_{t-t_{0}}^{(t)}<\omega\right\}\leq C_{0}\dot{\nu}^{t-2t_{0}}=O(\omega^{C_{2}})\dot{\nu}^{t}\ ,$$
t, (2.50)
where C2 := 2 + 2| log ˆν| log ν. Let γ := e
−aHt ˆ. It follows from (2.49) and (2.50) that

Pt {G c t (γ), Xt < ω} ≤ Pt {G c t (γ), Xt < ω, E1} + Pt {Xt < ω, Ec 1} ≤ P {G c t (γ) | E2} Pt {E2} + ωνˆ 2t ≤ O(ω C2)ˆν t ωX−1 j=1 X j (2.51) i=1 P {Xt = j, Γi,t ≤ γ | E2} + ωνˆ 2t ≤ O(ω C2+1)ˆν tP {Γ1,t < γ | E2} + ωνˆ 2t,
$$(2.49)$$
$$(2.50)$$
where the two last lines use a union bound and the symmetry of all individuals in generation t.

Thus, it suffices to upper bound the probability of [Γ1,t < γ] conditioned on E2. Let At,ω be the set of rooted trees T with width less than ω, height exactly t and such that the spine associated to each leaf has at most `(t) ramifications. The trees in At,ω are the candidates for GWt - the tree induced by the first t generations of the process (X
(t)
r )t≥r≥0, conditioned on E2. We will obtain an upper bound for the probability of [Γ1,t < γ] conditioned on E2 ∩ [GWt
∼= T], uniform for all T ∈ At,ω, which will also be an upper bound of P {Γ1,t < γ | E2}.

Let y0*, . . . , y*t be the individuals of the spine associated to x1. If r ∈ {0*, . . . , h* − 1} is not a ramification of T, we first sample ˜ξ
(t−r), the number of children of yr in (Xr)r≥0. Then conditioned on ˜ξ
(t−r), we sample the mark of yr. If r ∈ {0*, . . . , h* − 1} is a ramification of T, or if r ∈ {*h, . . . , t*},
we simply give yr the mark M. This procedure gives a stochastic lower bound of Γ1,t.

Similar to ˜ξ defined in (2.12), the distribution of ˜ξ
(a)is given by

$$\mathbb{P}\left\{\tilde{\xi}^{(a)}=k\right\}=\mathbb{P}\left\{X_{1}=k\ \Big{|}\ X_{1}^{(a)}=1\right\}=\frac{ks_{a-1}(1-s_{a-1})^{k-1\mathbb{P}}\left\{\xi=k\right\}}{s_{a}\mathbb{P}\left\{\xi^{(a)}=1\right\}}=\mathbb{P}\left\{\tilde{\xi}=k\right\}+O(\tilde{\nu}^{a})\,\tag{2.52}$$

for k ≥ 1. Therefore, we can couple ˜ξ
(t)*, . . . ,*
˜ξ
(t−(h+1)) with ˜ξ1*, . . . ,*
˜ξh, which are iid copies of ˜ξ, such that P
n˜ξ
(t−r) 6= ˜ξr+1o< ω−1. Thus, the number of positions where the two sequences differ is stochastically bounded from above by a binomial random variable with parameters (*h, ω*−1).

Let E3 be the event that (˜ξ
(t)*, . . . ,*
˜ξ
(t−(h+1))) and (˜ξ1, . . . ,
˜ξh) differ at at least m(t) := t(log t)
−1/2 positions. It follows that

$$\mathbb{P}\left\{E_{3}^{c}\right\}\leq(h/\omega)^{m(t)}\leq t^{-m(t)}=e^{-t{\sqrt{\log t}}}\;,$$
√log t, (2.53)
where we used that *h < t* ≤
√ω. Thus, we obtain

$$\mathbb{P}\left\{\Gamma_{1,t}<\gamma\mid E_{2}\cap[G W_{t}\cong T]\right\}\leq\mathbb{P}\left\{\Gamma_{1,t}<\gamma\mid E_{2}\cap E_{3}\cap[G W_{t}\cong T]\right\}+e^{-t{\sqrt{\log t}}}\;.$$
√log t. (2.54)
Let (˜ζr)r≥0 be iid copies of ˜ζ as defined in Subsection 2.2.2. Conditioning on E2∩E3∩[GWh
∼= T],
we have

$$\Gamma_{1,t}\succeq M^{-(\ell(t)+t_{0}+m(t))}\prod_{r=0}^{t-1}(\tilde{\zeta}_{r})^{-1}\ .$$
$$(2.53)$$
r=0
−1. (2.55)
where  denotes stochastical domination. It follows from Cram´er's theorem (Theorem 2.4) that

$$\mathbb{P}\left\{\Gamma_{1,t}<e^{-a\hat{H}t}\,\Bigm{|}E_{2}\cap E_{3}\cap[G W_{t}\cong T]\right\}$$ $$\leq\mathbb{P}\left\{\frac{\sum_{r=0}^{t-1}\log\check{\zeta}_{r}}{t}>a\hat{H}-\frac{(\ell(t)+t_{0}+m(t))\log M}{t}\right\}$$ $$\leq\exp\left\{-\left(I\left(a\hat{H}-\frac{(\ell(t)+t_{0}+m(t))\log M}{t}\right)+o(1)\right)t\right\}$$ $$=e^{-(I(a\hat{H})+o(1))t}\;,$$
$$(2.54)$$
$$(2.55)$$
$$(2.56)$$
$$(2.57)$$
$$(2.58)$$

where in the last step we use that `(t), m(t), t0 = o(t), Hˆ ≤ *aH <* ˆ log M, I(z) is continuous on
[H, ˆ ∞), and I
0(z) < ∞ for all z > Hˆ . Putting this into (2.54), we have

$$\mathbb{P}\left\{\Gamma_{1,t}<e^{-a{\hat{H}}t}\;\bigg|\;E_{2}\cap[G W_{t}\cong T]\right\}\leq e^{-(I(a{\hat{H}})+o(1))t}\;.$$

Theorem 2.6 follows by putting the above into (2.51).

## 2.5.5 A Corollary

The following corollary is convenient for our later use. Corollary 2.9. *Similarly as in* (1.10)*, consider*

$$\phi(a):=\frac{1}{a}\left(|\log\hat{\nu}|+I(a\hat{H})\right)\;.$$
. (2.58)
and a0 its minimum in [0, ∞) which satisfies a0 ≥ 1. For any p → 0*, let* ω ∈ (| log p| 3, e| log p| 1/3)
and γp = e
−Hˆ |log p|/φ(a0)*. We have*

$$\mathbb{P}\left\{G_{t_{\omega}}^{c}\left(\gamma_{p}\right)\cap\left[t_{\omega}<\infty\right]\right\}\leq p^{1+o(1)}\ .$$
1+o(1). (2.59)
In particular, if p = n
−βfor β > 0*, we have* γp = n
−*βH/φ* ˆ (a0).

$$(2.59)$$

Proof. For t ≤ log2 ω, we deterministically have

$$M^{-t}>M^{-\log^{2}\omega}>\gamma_{p}\;,$$
M−t > M− log2 ω > γp , (2.60)
which implies

$$\mathbb{P}\left\{G_{t}^{c}(\gamma_{p})\right\}=\mathbb{P}\left\{\cup_{i=1}^{X_{t}}\left[\Gamma_{i,t}\leq\gamma_{p}\right]\right\}\leq\mathbb{P}\left\{\cup_{i=1}^{X_{t}}\left[M^{-t}\leq\gamma_{p}\right]\right\}=0\;.$$
o= 0 . (2.61)
Let t∗ = |log p|/|log ˆν| <
√ω. Choose t ∈ (log2 *ω, t*∗] and set a :=
|log p| tφ(a0)
. As φ(a0) ≤ φ(1) = |log ˆν|, we have a ≥ 1. It follows from Theorem 2.6 that

$$\mathbb{P}\left\{G_{t}(\gamma_{p})^{c}\cap[0<X_{t}<\omega]\right\}=\mathbb{P}\left\{G_{t}\left(e^{-a H t}\right)^{c}\cap[0<X_{t}<\omega]\right\}$$ $$\leq\exp\biggl\{-(1+o(1))\frac{|\log p|\phi\left(a\right)}{\phi\left(a_{0}\right)}\biggr\}\leq p^{1+o(1)}\ ,$$
$$(2.61)$$
$$(2.62)$$

where the last step uses the fact that a0 minimises φ(a) for all a ≥ 0. Thus,

$$\mathbb{P}\left\{G_{t_{\omega}}^{c}(\gamma_{p})\cap[t_{\omega}<\infty]\right\}=\sum_{t\geq0}\mathbb{P}\left\{G_{t_{\omega}}(\gamma_{p})^{c},t_{\omega}=t+1\right\}$$
$$\begin{array}{l}{{\leq\sum_{t=\log^{2}\omega}^{t_{\star}}\mathbb{P}\left\{G_{t}(\gamma_{p})^{c},0<X_{t}<\omega\right\}+\mathbb{P}\left\{t_{\omega}\geq t_{\star}\right\}}}\\ {{\leq t_{\star}p^{1+o(1)}+C_{0}\hat{\nu}^{t_{\star}-2t_{0}}}}\\ {{\leq p^{1+o(1)}\;,}}\end{array}$$

where we used Lemma 2.7 to bound P {tω > t∗}, as in (2.50).

## 2.6 A Truncated Martingale

We will consider a truncated version of Γt defined in (2.4). Fix t0 ∈ N and γ > 0. Recall that
(f t−t0 (i, t), t0) is the ancestor of the node (*i, t*) in generation t0. For every t ≥ t0 and i ∈ [Xt],
define

$$\hat{\Gamma}_{i,t}:=\gamma\left(\prod_{r=1}^{t-t_{0}}\zeta_{t^{r-r}(i,t),r}\right)^{-1}\,,\qquad\hat{\Gamma}_{t}:=\sum_{i\in[X_{t}]}\hat{\Gamma}_{i,t}\;.\tag{2.63}$$

Given that Γi,t0 ≥ γ for all i ∈ [Xt0
], we have Γˆi,t ≤ Γi,t for i ∈ [Xt] and Γˆt ≤ Γt. By the same argument of Lemma 2.1, (ΓˆtE[ξ/ζ]
−(t−t0))t≥t0 is also a martingale with respect to (Fr)r≥t0
.

Proposition 2.10. Let (Xr)r≥0 *be a marked branching process with offspring distribution* η = (*ξ, ζ*)
with E[ξ] ∈ (1, ∞). Let M ∈ N and ω → ∞. Suppose that |E[ξ/ζ] − 1| ≤ Mω−1/3, ξ ≤ M and ζ ≥ 1. There exists a constant c0 > 0 such that for any t0 ∈ N and t ∈ [t0, ω1/4] *we have*

$$\mathbb{P}\left\{\hat{\Gamma}_{t}\geq\omega\gamma/2\,\bigg|\,\left[X_{t_{0}}\geq\omega\right]\cap G_{t_{0}}(\gamma)\right\}\geq1-e^{-c_{0}\omega^{1/3}}\;.$$
1/3. (2.64)
Proof. Consider the collection of events Er = [Γˆr/Γˆt0 ∈ (1/2, 3/2)] r ∈ (t0, t] . (2.65)

$$(2.64)$$

17 We will lower bound the probability of Er using Azuma's inequality, see, e.g., [21, Chapter 11].

Note that, given Fr, m := Xr and Γˆr are measurable. Let T1, T01
, . . . , Tm, T0m be iid marked Galton-Watson trees with offspring η, where Tiis rooted at node i of generation r. Given Fr, Γˆr+1 = g(T1*, . . . , T*m), where g is a function depending on Fr. Note that for every choice of T1*, . . . , T*m and T
0 i
,

$$|g(T_{1},\ldots,T_{i},\ldots T_{m})-g(T_{1},\ldots,T_{i}^{\prime},\ldots,T_{m})|\leq M\hat{\Gamma}_{i,r}\leq M\gamma\ ,$$

since the tree Ti contributes to Γˆr+1 with at most

$$\sum_{j\geq0}{\bf1}_{f(j,r+1)=i}\hat{\Gamma}_{j,r+1}=\hat{\Gamma}_{i,r}\sum_{j\geq0}\frac{{\bf1}_{f(j,r+1)=i}}{\zeta_{j,r+1}}\leq\xi_{i,r}\hat{\Gamma}_{i,r}\leq M\hat{\Gamma}_{i,r}\ .$$

Since (ΓˆtE[ξ/ζ]
−(t−t0))t≥t0 is a martingale, we have

$$\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\hat{\Gamma}_{r+1}\,\left|\,\mathcal{F}_{r}\right.\right]\right|=\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\xi/\zeta\right]\hat{\Gamma}_{r}\right|\leq M\omega^{-1/3}\hat{\Gamma}_{r}=:s\;.$$

Thus, it follows from (2.68) and Azuma's inequality [21, pp. 92] that

$$\mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\hat{\Gamma}_{r}\right|>2s\ \right|\ \mathcal{F}_{r}\right\}$$ $$\leq\mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\mathbb{E}\left[\hat{\Gamma}_{r+1}\ \right|\ \mathcal{F}_{r}\right]\right|>s\ \right|\ \mathcal{F}_{r}\right\}+\mathbb{P}\left\{\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\hat{\Gamma}_{r+1}\ \right|\ \mathcal{F}_{r}\right]\right|>s\ \right|\ \mathcal{F}_{r}\right\}$$ $$\leq2\exp\!\left\{-\frac{2s^{2}}{\sum_{i=1}^{m}(M\hat{\Gamma}_{i,r})^{2}}\right\}.$$
$$(2.66)$$
$$(2.67)$$
$$(2.68)$$
$$(2.69)$$
$$(2.70)$$

Note that Pm i=1(MΓˆi,r)
2 ≤ γM2 Pm i=1 Γˆi,r = γM2Γˆr. Thus on the event Er, we have

$$\mathbb{P}\left\{|\hat{\Gamma}_{r+1}-\hat{\Gamma}_{r}|>2s\,\left|\,\mathcal{F}_{r}\right.\right\}\leq2\exp\left\{-\frac{2\omega^{-2/3}\hat{\Gamma}_{r}^{2}}{\gamma\hat{\Gamma}_{r}}\right\}\leq2e^{-\omega^{1/3}}\;,$$
$$(2.71)$$
1/3, (2.70)
where we used Γˆr ≥ ωγ/2 on Er in the last inequality.

Let Ft0 = [Xt0 ≥ ω] ∩ Gt0
(γ) and let Fr = [|Γˆr+1/Γˆr − 1| ≤ Mω−1/3] for *r > t*0. By (2.70),
there exists c0 > 0 such that

$$\mathbb{P}\left\{\cup_{r=t_{0}+1}^{t}F_{r}^{c}\mid F_{t_{0}}\right\}\leq\sum_{r=t_{0}}^{t-1}\mathbb{P}\left\{F_{r+1}^{c}\mid\cap_{t_{0}\leq s\leq r}F_{s}\right\}\leq2t e^{-\omega^{1/3}}\leq e^{-c_{0}\omega^{1/3}},$$

where we used ∩s≤rFs ⊆ Er. So, with the desired probability

$$\hat{\Gamma}_{t}\geq(1-2M\omega^{-1/3})^{t}\omega\gamma\geq\frac{\omega\gamma}{2}\;,$$
2, (2.71)
as t ≤ ω 1/4.

## 3 Exploring The Graph 3.1 The Exploration Process

In this section, we introduce a marked version of the Breadth First (BFS) Search graph exploration process defined in [9].

For a set of vertices I ⊆ [n], let E
±(I) be the tails/heads incident to I. Let E
± be the set of all tails/heads. For a set of half-edges X , let V(X ) be the vertices incident to X . For e
± ∈ E±, we use v(e
±) to denote the vertex incident to e
±.

We start from an arbitrary head e
−
0 ∈ E−. In this process, we create random pairings of halfedges one by one and keep each half-edge in exactly one of the three states - *active, paired*, or undiscovered. Let A
±
i
, P
±
iand U
±
idenote the set of tails/heads in the four states respectively after the i-th pairing of half-edges. Initially, let

$${\cal A}_{0}^{-}=\{e_{0}^{-}\},\ {\cal A}_{0}^{+}={\cal E}^{-}(v(e_{0}^{-})),\ {\cal P}_{0}^{\pm}=\emptyset,\ {\cal U}_{0}^{\pm}={\cal E}^{\pm}\setminus({\cal A}_{0}^{\pm}\cup{\cal P}_{0}^{\pm})\;.\tag{3.1}$$

Then set i = 1 and proceed as follows:
(i) Let e
−
ibe one of the heads which became active earliest in A
− i−1
.

(ii) Pair e
−
i with a tail e
+
ichosen uniformly at random from E
+ \ P+
i−1
. Let vi = v(e
−
i
) and P
±
i = P
±
i−1 ∪ {e
±
i
}.

(iii) If e
+
i ∈ A+
i−1
, then A
±
i = A
±
i−1
\ {e
± i
}; and if e
+
i ∈ U+
i−1
, then A
±
i = (A
±
i−1 ∪ E±(vi)) \ {e
± i
}.

(iv) If A
−
i = ∅, terminate; otherwise, let U
±
i =E
± \ (A
±
i ∪ P±
i
), i = i + 1 and go to (i).

If we are in the first case of step (iii), we say that a *collision* has happened. If there is no collision in the process up to certain time, the exposed in-neighbourhood of e
−
0 is a tree.

In parallel to the BFS process, we construct a tree with nodes corresponding to heads in the in-neighbourhood of e
−
0
. Write f = e
−
0and let T
−
f
(0) be a tree with one root node corresponding to f. Given T
−
f
(i − 1), T
−
f
(i) is constructed as follows: if e
−
i ∈ U −
i−1
, then construct T
−
f
(i) from T
−
f
(i − 1) by adding |E−(vi)| child nodes to the node representing e
−
i
, each of which representing a head in E
−(vi); otherwise, let T
−
f
(i) = T
−
f
(i − 1). See Figure 1 for an example the exploration process and its associated tree.

The nodes in T
−
f
(i) correspond to the heads in P
−
i ∪ A−
i
. So we can assign a label *paired* or active to each node of T
−
f
(i). We also give the node corresponding to e
−
ia *mark* which equals the out-degree of vi.

For half-edges e1 and e2, we define the *distance* from e1 to e2, denoted by dist(e1, e2), to be the length of the shortest path from v(e1) to v(e2) which starts with the edge containing e1 if e1 is a tail, and which ends with the edge containing e2 if e2 is head. For example, in Figure 1, dist(e
− 2
, e−
1
) = dist(e
+ 2
, e+
1
) = 1, dist(e
+ 3
, e−
1
) = 2, dist(e
− 4
, e+
1
) = 0.

If itis the last step where a head at distance t from f is paired, then T
−
f
(it) satisfies: (i) the height is t; and (ii) the set of actives nodes is the t-th level. We call a rooted tree T *incomplete* if it satisfies (i)-(ii). We let p(T) be the number of *paired* nodes in T.



## 3.2 Coupling The Exploration And Branching Processes

We will couple a marked incomplete tree constructed from the exploration process with a marked branching process with offspring distribution close to η = (*ξ, ζ*)
L= Dout. Thus we assume that ξ ≤ M, 2 ≤ ζ ≤ M and E [ξ/ζ] = 1 (by Remark 2.2).

We will need two slightly perturbed versions of η. Let β ∈ (0, 1/4). Consider the probability distribution η
↓ = η
↓(β) = (ξ
↓(β), ζ↓(β)) defined by

$$\mathbb{P}\left\{\eta^{\mathbb{I}}=(k,\ell)\right\}\coloneqq\begin{cases}c^{\mathbb{I}}\mathbb{P}\left\{\eta=(k,\ell)\right\}&\text{if}\mathbb{P}\left\{\eta=(k,\ell)\right\}\geq n^{-1+\beta}\\ 0&\text{otherwise}\end{cases}\tag{3.2}$$

where c
↓is a normalising constant. It is easy to check that our assumptions on η implies that c
↓ = 1 + O(n
−1+β).

Similarly, the probability distribution η
↑ = η
↑(β) is defined by

$$\mathbb{P}\left\{\eta^{\uparrow}=(k,\ell)\right\}:=\begin{cases}c^{\dagger}\mathbb{P}\left\{\eta=(k,\ell)\right\}&k\geq1\\ c^{\dagger}\mathbb{P}\left\{\eta=(0,\ell)\right\}+n^{-1+\beta}&k=0\end{cases}\tag{3.3}$$

where c
↑ = 1 − O(n
−1+β) is a normalising constant.

One can show that η
↓satisfies |E[ξ
↓/ζ↓] − 1| = O(n
−1+β) and similarly for η
↑. If ω is polylogarithmic, it will be possible to apply Proposition 2.10 to them.

Let GWη be a marked Galton-Watson tree with offspring distribution η. For a marked incomplete rooted tree T, we use the notation GWη
∼= T to denote that T is isomorphic to a root subtree of GWη, i.e., the degree of paired nodes of T agrees with GWη, and all marks in T and GWη agree. For a set of incomplete trees T , let [GWη ∈ T ] := ∪T ∈T [GWη
∼= T] and
[T
−
f ∈ T ] := ∪T ∈T [T
−
f
(p(T)) = T]. The following is a simple adaption of [9, Lemma 5.3]; we omit the proof.

Lemma 3.1. Let β ∈ (0, 1/4) and let T be a set of incomplete trees such that p(T) < n β 10 *for all* T ∈ T *. We have*

$$(1+o(1))\,\mathbb{P}\left\{\text{GW}_{\eta^{1}(\beta)}\in\mathcal{T}\right\}\leq\mathbb{P}\left\{T_{f}^{-}\in\mathcal{T}\right\}\leq(1+o(1))\,\mathbb{P}\left\{\text{GW}_{\eta^{1}(\beta)}\in\mathcal{T}\right\}.\tag{3.4}$$

## 4 Stationary Distributions

We will proceed to prove Theorem 1.1 as in [11], using the results obtained in previous sections.

## 4.1 The Largest Strongly Connected Component

For δ
+ ≥ 2, whp there is a linear size scc in G~n [8, 14]. We will first show that whp this component is attractive (and so unique), which implies that the simple random walk on G~n has a unique stationary distribution whp.

For a tail/head e
± ∈ E±, let N
±
k
(e
±) and N
±
≤k
(e
±) be the sets of tails/heads at distance k and at most k from/to e
±, respectively; that is,

$$\mathcal{N}_{k}^{+}(e^{+})=\left\{f^{+}\in\mathcal{E}^{+}:\mathrm{dist}(e^{+},f^{+})=k\right\}\,\mathcal{N}_{k}^{-}(e^{-})\coloneqq\left\{f^{-}\in\mathcal{E}^{-}:\mathrm{dist}(f^{-},e^{-})=k\right\}\,$$ $$\mathcal{N}_{\leq k}^{+}(e^{+})\coloneqq\left\{f^{+}\in\mathcal{E}^{+}:\mathrm{dist}(e^{+},f^{+})\leq k\right\}\,\mathcal{N}_{\leq k}^{-}(e^{-})\coloneqq\left\{f^{-}\in\mathcal{E}^{-}:\mathrm{dist}(f^{-},e^{-})\leq k\right\}.\tag{4.1}$$

Similarly, for a vertex x ∈ [n], let N
±
k
(x) and N
±
≤k
(x) be the sets of vertices at distance k and at most k from/to x, respectively.

Throughout this section, let ω := log6 n. For every tail/head e
± ∈ E±, consider the stopping time t
± ω
(e
±) := inf{t ≥ 0 :N
±) ≥ ω} ; (4.2)
that is, the first time when there are at least ω half-edges in the in/out-edge neighbourhood of e
±.

$$\left|{\mathcal N}_{t}^{\pm}(e^{\pm})\right|\geq\omega\}\;;$$

Proposition 4.1. Let M ∈ N *and suppose that* δ
+ ≥ 2 and ∆± ≤ M. Let C0 denote the largest strongly connected component in G~n*. Let*

$\star\pm\epsilon$ ... 
$${\mathcal{E}}_{0}^{-}:=\{f\in{\mathcal{E}}^{-}:t_{\omega}^{-}(f)<\infty\}\;.$$
(f) < ∞} . (4.3)
Let E0 be the event that C0 is attractive (i.e., there is a directed path from every vertex to C0) and has vertex set V0 = V(E
−
0
)*. Then* P {E0} = 1 − o(1). Thus, whp the simple random walk on G~n has a unique stationary distribution supported on V0.

Proof. Let h := 1 + log2
(2ω) = O(log log n). Then, it follows from [11, Lemma 2.2] that

$$\mathbb{P}\left\{\cup_{e\in\xi^{+}}[t_{\omega}^{+}(e)>h]\right\}\leq\mathbb{P}\left\{\cup_{y\in[n]}\left[|\mathcal{W}_{h-1}^{+}(y)|\leq\omega\right]\right\}\tag{4.4}$$ $$\leq\mathbb{P}\left\{\cup_{y\in[n]}\left[\mathcal{W}_{h-1}^{+}(y)|\leq\frac{1}{2}(\delta^{+})^{h-1}\right]\right\}=o(n^{-1})\;,$$
$\left(4.3\right)$ . 
$$21$$

where we used |N +
h
(e)*| ≥ |N* +
h−1
(y)|, where y is the vertex incident to the head paired with e. So, t
+
ω
(e) ≤ h for all e ∈ E+ whp.

By [9, Lemma 6.2]2, whp every f ∈ E− either has t
−
ω
(f) = ∞ or t
−
ω
(f) = O(log n). Conditioning on t
+
ω
(e) = O(log log n) and t
−
ω
(f) = O(log n), [9, Proposition 7.2] implies that there is a path from e to f with probability 1 − o(n
−2). Thus, by a union bound over all choices of e and f, we have that whp there is path from every e ∈ E+ to every f ∈ E−
0
; and in particular, from every x ∈ [n]
to every y ∈ V0. In other words, whp C0 contains all vertices in V0 and is attractive. Using [9, Proposition 6.1] with t = n/ω, it is easy to check that whp for every x ∈ [n] \ V0, there exists yx such that yx cannot be reached from x. It follows that whp C0 has vertex set V0.

## 4.2 Random Walk On Heads

It will be more convenient to perform a random walk on heads instead of vertices. Consider the
random process (Z
e
t
)t≥0 with state space E
− and, conditioning on G~n and Z
e
t = f, let Z
e
t+1 be
chosen uniformly at random among all heads paired with tails of v(f), the endpoint of f. It follows from Proposition 4.1 that (Z
e
t
)t≥0 has a unique stationary distribution supported on E
−
0
, denoted
by π
e.
Recall that πmin := min{π(y) : y ∈ [n], π(y) > 0}. Let π
e
min := min{π
e(f) : f ∈ E−, πe(f) > 0}.
Define
$$\pi_{0}:=\operatorname*{min}\{\pi(y):\,y\in\mathcal{V}_{0}\}\;,\qquad\pi_{0}^{\mathsf{e}}:=\operatorname*{min}\{\pi^{\mathsf{e}}(f):\,f\in\mathcal{E}_{0}^{-}\}\;.$$
} . (4.5)
By Proposition 4.1, whp π0 = πmin and π e 0 = π e min. The following lemma shows that whp π e 0 differs from π0 by a bounded factor. Thus, it suffices to prove Theorem 1.1 for π e 0
.

Lemma 4.2. Whp π e 0 ≤ π0 ≤ Mπe 0
.

Proof. Let V0 and E
−
0and E0 be as in Proposition 4.1. Since E0 happens whp, it suffices to prove the lemma conditioning on G~n ∈ E0. So we may assume there is a unique stationary distribution π and limt→∞ P {Zt = y | Z0 = x} = π(y), uniformly for all x ∈ [n].

Let *x, y* ∈ [n] and e ∈ E−(x). Note that (v(Z
e t
))t≥0 (i.e., the endpoints of the heads in (Z
e t
)t≥0)
is itself a simple random walk on [n] with the same law as (Zt)t≥0. Therefore,

$$\mathbb{P}\left\{Z_{t}=y\mid Z_{0}=x\right\}=\sum_{f\in\mathcal{E}^{-}(y)}\mathbb{P}\left\{Z_{t}^{\mathsf{e}}=f\mid Z_{0}^{\mathsf{e}}=e\right\}\;.$$

If y ∈ V0, then letting t → ∞ on both sides we have

$$\pi(y)=\sum_{f\in{\mathcal E}^{-}(y)}\pi^{\mathbf e}(f)\geq\pi_{0}^{\mathbf e}\;,$$
$\left(4.5\right)^{2}$
$$(4.6)$$
$$(4.7)$$

where the last inequality holds by definition of V0. Since y ∈ V0 is arbitrary, it follows that π0 ≥ π e 0
.

For the other direction, let f0 ∈ E−
0 with π e(f0) = π e 0
. Let z be the vertex that has a tail paired with f0. Let e ∈ E− and let x be its endpoint. We have

$$\mathbb{P}\left\{Z_{t}^{\mathbf{e}}=f_{0}\mid Z_{0}^{\mathbf{e}}=e\right\}={\frac{1}{d_{z}^{+}}}\mathbb{P}\left\{Z_{t-1}=z\mid Z_{0}=x\right\}\ .$$
P {Zt−1 = z | Z0 = x} . (4.8)
$$(4.8)$$

Again, letting t → ∞ we have

$$\pi_{0}^{\bf e}=\pi^{\bf e}(f)=\frac{1}{d_{z}^{+}}\pi(z)\geq\frac{1}{M}\pi_{0}.\tag{4.9}$$
$$\lceil\!\!\!\!\perp\!\!\!\perp$$

Thus π0 ≤ Mπe 0
.

## 4.3 Lower Bound For Π E

0 To prove a lower bound it will suffice to lower bound the probability of reaching a particular head f ∈ E−
0
, uniformly for all starting points e ∈ E− (see Lemma 4.4 below). We use the ideas introduced in [5, 6, 11] to capture the weight (defined below) of typical trajectories departing from e. Our main contribution is to control the total weight of the trajectories landing at f.

Define the *out-entropy* H+ and the *entropic time* τ ent as

$$H^{+}:=\frac{1}{m}\sum_{x\in[n]}d_{x}^{-}\log d_{x}^{+},\quad\tau_{\mathrm{ent}}:=\frac{\log n}{H^{+}}\ .$$
$$(4.10)$$

We stress the similarity between H+ and Hˆ − defined in (1.8); in particular, H+ = E[log D
+
in]. In this case, the out-entropy is related to typical trajectories in supercritical out-growth.

Fix ε > 0 small enough and let

$$h^{+}:=(1-\varepsilon)\tau_{\mathrm{ent}},\qquad h^{-}:=\frac{3\varepsilon}{\log\delta^{+}}\log n\ .$$
For $f\in\mathcal{E}^{-}$, let $h(f):=t_{\omega}^{-}(f)\wedge\omega.$ Define. 
$$(4.11)$$

$$(4.12)$$
$$\tau(f):=h^{+}+h^{-}+h(f)\ .$$
− + h(f) . (4.12)
Remark 4.3. Bordenave, Caputo and Salez [5, 6] showed that the mixing time of the random walk on G~n coincides with the *entropic time*, and exhibits cutoff. Jensen's inequality implies that τ ent ≥ (1 + o(1)) logν n, which is whp the typical distance of the G~n. Thus, by the results in [9],
τ ent + maxf∈E−
0 h(f) is at least the diameter of G~n whp.

Throughout this section, we will use letters *a, b* for tails in E
+ and letters *e, f, g* for heads in E
−. Let P
t(*e, f*) := P {Z
e t = f | Z
e 0 = e}. To lower bound π e 0 it suffices to prove the following:
Lemma 4.4. With high probability, for all e ∈ E− and f ∈ E−

$$\in{\mathcal{E}}^{-}\,\,\,a n d\,\,f\in{\mathcal{E}}_{0}^{-}\,,$$
$$P^{\tau(f)}(e,f)\geq n^{-(1+\hat{H}^{-}/\phi(a_{0})+o(1))}\ .$$
−(1+Hˆ −/φ(a0)+o(1)). (4.13)
$$\mathrm{Let}\ f\in{\mathcal{E}}_{0}^{-}\,.$$

. Assuming Lemma 4.4 and by stationarity at time τ (f), whp we have

$$\pi^{\mathbf{e}}(f)=\sum_{e\in{\mathcal E}^{-}}\pi^{\mathbf{e}}(e)P^{\tau(f)}(e,f)\geq n^{-(1+{\hat{H}}^{-}/\phi(a_{0})+o(1))}\ ,$$

and the lower bound in Theorem 1.1 follows.

$$(4.13)$$

$$(4.14)$$

From now on, we fix two heads e, f ∈ E−. We sequentially expose the out-neighbourhood of e (out-phase) and the in-neighbourhood of f (in-phase) as explained below. To control the trajectories departing from e, we will use the idea of *nice paths* introduced in [5] as described in [11].

In the out-phase we build a directed rooted tree T
+
e
, partially exposing the out-neighbourhood of e. The root of T
+
erepresents the head e and all its other nodes represent tails in E
+. For a tail a represented in T
+
e
, let h(a) denote its height in T
+
e
. Define its *weight* by

$$\mathbf{w}(a):=\prod_{i=1}^{\mathbf{h}(a)}{\frac{1}{d_{v(a_{i})}^{+}}}\ .$$
$$(4.15)$$

$\frac{\pi}{1}$ = . 
. (4.15)
where a1*, . . . , a*h(a) are the tails in the path from e to a in T
+
e
.

To construct T
+
e we use a procedure similar to Subsection 3.1, with the roles of heads and tails reversed and the following two modifications:
a) There is no initial tail e
+ 0
. We start at step i = 2 with e
−
1 = e and A
±
1 = E
±(v(e)).

b) For each i ≥ 2, at step (i) we choose e
+
ito be the tail in A
+
i−1 that *maximises* w(a) among all a ∈ A+
i−1 with h(a) ≤ h
+ − 1 and w(a) ≥ n
−1+ε 2.

The tree T
+
eis not constructed in a BFS manner, but maximising the weight of its nodes; by construction, h(a) ≥ dist(*e, a*)+1. The process stops if there are no more eligible tails. In [6, Lemma 7], the authors showed that *deterministically* constructing T
+
eexposes at most κ
+ := n 1−ε 2/2edges.

Moreover, if g is the head paired with a tail a represented in T
+
e
, we have P
h(a)(*e, g*) ≥ w(a).

In the in-phase we build a directed rooted tree T
−
f
, exposing N
−
≤h−+h(f)
(f) conditionally on T
+
e
.

We use the exploration process defined in Subsection 3.1 with one modification: if e
−
ihas already been paired with e
+ in the exploration of T
+
e
, we let e
+
i = e
+. We stop once all heads at distance h
− + h(f) from f have been activated and we let T
−
fbe the final incomplete tree generated up to this point. For a head g in T
−
f
, let h(g) be its height in T
−
f
; note that h(g) = dist(*g, f*). We define its *weight* by

its weight by  $${\bf w}(g)\coloneqq\prod_{i=1}^{{\sf h}(g)}\frac{1}{d_{v(g)}^{+}}\;,\tag{4.16}$$  where $f=g_{0},\ldots,g_{{\sf h}(g)}$ are the heads in the path from $f$ to $g$ in $T_{f}^{-}$. Similarly as before,
$$(4.16)$$
$$(4.17)$$
P
h(g)(*g, f*) ≥ w(g). For any α > 0, we can let ε be small enough such that the number of edges exposed in the in-phase is at most

$$\kappa^{-}:=\omega h(f)+\omega(\Delta^{-})^{h^{-}+1}=n^{3(\log\Delta^{-}/\log\delta^{+})\varepsilon+o(1)}=O(n^{\alpha})\;.$$
α) . (4.17)
Let σ0 be a partial realisation of the configuration model in which at most κ := κ
+ + κ
− edges are revealed during the out- and in-phases. Then T
+
e
, T −
f
, h(f), N
−
≤h−+h(f)
(f) and τ (f) are all measurable with respect to σ0. Given σ0, let A be the set of unpaired tails a in T
+
e with h(a) = h
+,
and let G be the set of unpaired heads g in T
−
f which satisfy h(g) = h
− + h(y).

To bound the desired probability, it will be convenient to consider trajectories that are not too heavy. Following the ideas in [5, 6, 11], it suffices to only consider *nice paths* in T
−
f
; that is, we restrict our attention to the tails a ∈ A such that w(a) ≤ n
−1+2. For heads in G, we define a truncated version of the weight. Let γ := n
−(1+ε)Hˆ −/φ(a0). For g in T
−
f with h(g) ≥ h(f), we define its *truncated weight* by

$${\hat{\mathbf{w}}}(g):=(\mathbf{w}({\hat{g}})\wedge\gamma)\prod_{i=h(f)+1}^{\mathbf{h}(g)}{\frac{1}{d_{v(g_{i})}^{+}}}\ .$$
$$(4.18)$$
$$(4.19)$$
$$(4.20)$$

where ˆg is the unique head in the path from f to g in T
−
f with h(ˆg) = h(f). In words, the truncated weight caps the contribution of the last h(f) steps by γ. By the definition of h
− in (4.11), for g ∈ G
we have

$$\hat{\mathbf{w}}(g)\leq(\delta^{+})^{-h^{-}}\gamma\leq\gamma n^{-3\varepsilon}\ .$$
−h−γ ≤ γn−3ε. (4.19)
Let σ be a complete pairing of half-edges compatible with σ0. Let [σ(a) = g] be the event that a and g are paired in σ. Conditioning on σ0, we can lower bound the desired probability as follows

$$P^{r(f)}(e,f)\geq\hat{P}^{r(f)}(e,f):=\sum_{a\in{\mathcal{A}}}\sum_{g\in{\mathcal{G}}}{\bf w}(a)\hat{\bf w}(g){\bf1}_{\sigma(a)=g}{\bf1}_{{\bf w}(a)\leq n^{-1+2\varepsilon}}\ .$$

Let
$$A_{e,f}(\sigma_{0}):=\sum_{a\in{\mathcal A}}{\bf w}(a){\bf1}_{{\bf w}(a)\leq n^{-1+2\epsilon}}\;,\qquad B_{e,f}(\sigma_{0}):=\sum_{g\in{\mathcal G}}\hat{\bf w}(g)\;,$$
ˆw(g) , (4.21)
which are measurable with respect to σ0. Consider the event

$${\mathcal{V}}_{e,f}=\left\{\sigma_{0}:\,A_{e,f}(\sigma_{0})\geq{\frac{1}{2}},\;B_{e,f}(\sigma_{0})\geq{\frac{\omega\gamma}{4}}\;\right\}\;.$$
. (4.22)
$${\mathrm{If~}}\sigma_{0}\in{\mathcal{Y}}_{e,f},{\mathrm{~then}}$$
$$\mathbb{E}\left[\hat{P}^{\tau(f)}(e,f)\;\Big\vert\;\sigma_{0}\right]\geq\frac{1}{m}A_{e,f}(\sigma_{0})B_{e,f}(\sigma_{0})\geq\frac{\omega\gamma}{8m}\geq\frac{2\gamma}{n}\;.$$
n. (4.23)
We prove a concentration result similar to [11, Lemma 3.6], exploiting the truncated nature of Pˆt:
Lemma 4.5. For every σ0 ∈ Ye,f *and every* a ∈ (0, 1)

$$\mathbb{P}\left\{\hat{P}^{\tau(f)}(e,f)\leq(1-a)\mathbb{E}\left[\hat{P}^{\tau(f)}(e,f)\;\Big\vert\;\sigma_{0}\right]\;\Big\vert\;\sigma_{0}\right\}\leq\exp\!\left(-\frac{a^{2}n^{e}}{3}\right)\;.$$
$$(4.21)$$
$$(4.22)$$
$$(4.23)$$
$$(4.24)$$

Proof. One can write Pˆτ(f)(*e, f*) = Pa∈E+ c(*a, σ*(a)) where c(*a, g*) = w(a)wˆ (g)1w(a)≤n2ε−1 1a∈A,g∈G . (4.24)

By (4.19), it follows that:  $\newcommand{\Bold}[1]{\mathbf{#1}}$ $\newcommand{\Bold}[1]{\mathbf{#1}}$
c(*a, σ*(a)) ≤ γn−(1+ε). (4.25)
$$\|c\|_{\infty}:=\operatorname*{max}_{a\in{\mathcal{E}}^{+}}c(a,\sigma(a))\leq\gamma n^{-(1+\varepsilon)}\ .$$
Using the one-sided version of Chariepe's inequality for uniformly random pairings [12, Proposition 1.1], we get that  $$\mathbb{P}\left\{\hat{P}^{r(f)}(e,f)\leq(1-a)\mathbb{E}\left[\hat{P}^{r(f)}(e,f)\ \Big{|}\ \sigma_{0}\right]\ \Big{|}\ \sigma_{0}\right\}\leq\exp\left(-\frac{a^{2}\mathbb{E}\left[\hat{P}^{r(f)}(e,f)\ \Big{|}\ \sigma_{0}\right]}{6\|e\|_{\infty}}\right)\leq e^{-\frac{a^{2}\sigma^{2}}{2}}\,$$  where we used (4.23) in the last line.  
$$(4.25)$$

25 Lemma 4.6. *Define the event*

$${\mathcal{V}}=\bigcap_{e,f\in{\mathcal{E}}^{-}}({\mathcal{V}}_{e,f}\cup[t_{\omega}^{-}(f)=\infty])\;.$$
$$(4.26)$$

We have P {Y} = 1 − o(1).

Let us prove Lemma 4.4 assuming that Lemma 4.6 holds. The proof of Lemma 4.6 is postponed to Subsection 4.4.

Proof of *Lemma 4.4.* Write

$$E_{f}:=[t_{\omega}^{-}(f)=\infty]\;,\qquad F_{e,f}:=\left[\dot{P}^{\tau(f)}(e,f)\geq\frac{\gamma}{n}\right]\;.$$  In the case $\omega$ is a $\omega$-function. The $\omega$-function is a $\omega$-function. The $\omega$-function is a $\omega$-function. The $\omega$-function is a $\omega$-function.  
i. (4.27)
Using (4.23) and applying Lemma 4.5 with $a=1/3$,  $$\mathbb{P}\left\{F_{e,f}^{c}\ \big{|}\ \mathcal{Y}_{e,f}\right\}=\mathbb{P}\left\{\hat{P}^{r(f)}(e,f)<\frac{\gamma}{n}\ \Big{|}\ \mathcal{Y}_{e,f}\right\}\leq\max_{\sigma_{0}\in\mathcal{Y}_{e,f}}\mathbb{P}\left\{\hat{P}^{r(f)}(e,f)<\frac{\gamma}{n}\ \Big{|}\ \sigma_{0}\right\}=o(n^{-2}).\tag{4.28}$$  It follows that,
P∩e,f∈E− (Fe,f ∪ Ef )	≥ PY ∩ (∩e,f∈E− (Fe,f ∪ Ef ))	 = P {Y} − PY ∩ ∩e,f∈E− (Fe,f ∪ Ef )c	 ≥ (1 − o(1)) −X e,f∈E− PY ∩ F c e,f ∩ E c f 	 ≥ (1 − o(1)) −X e,f∈E− PF c e,f ∩ Ye,f	 ≥ (1 − o(1)) −X e,f∈E− PF c e,f | Ye,f	 = 1 − o(1) ,
$$(4.27)$$
$$(4.29)$$

where we used that Y ∩ Ec f implies Ye,f .

So, whp, if t
−
ω
(f) < ∞, then Fe,f holds. In other words, we have shown that for all e ∈ E− and f ∈ E−
0
,

 $ P^{\tau(f)}(e,f)\geq\hat{P}^{\tau(f)}(e,f)\geq\frac{\gamma}{n}\geq n^{-(1+(1+\epsilon)\hat{H}^-/\phi(a_0))}.$  I'll say I'm all, this proves the lemma. 
Since ε can be arbitrarily small, this proves the lemma.

## 4.4 Proof Of Lemma 4.6

Define the following events

$${\cal A}_{1}=\bigcap_{e,f\in{\cal E}^{-}}\left[A_{e,f}\geq\frac{1}{2}\right]\,\qquad{\cal A}_{2}=\bigcap_{e,f\in{\cal E}^{-}}\left[B_{e,f}\geq\frac{\omega\gamma}{4}\right]\cup\left[t_{\omega}^{-}(f)=\infty\right]\,,\tag{4.30}$$  Let $\gamma\supset A_{1}\cap A_{2}$.  
and note that Y ⊇ A1 ∩ A2.

The vertex analogue of the event A1 was studied in [11, Lemma 3.7]. Note that its proof does not use any assumption on δ
−, is valid for tail-trees instead of vertex-trees and accommodates the κ
− = O(n α) half-edges revealed during the in-phase. Thus the conclusion P {A1} = 1 − o(1) still holds in our setting.

We are left with showing the following lemma:
Proof. In order to analyse the probability of A2, it will be convenient to swap the order of the phases: we first run the in-phase unconditionally, and then the out-phase. Write h = h(f) + h
−.

We will first prove that

$$\hat{\Gamma}_{h}^{\rm T}(f):=\sum_{g\in{\cal N}_{h}^{-}(f)}\hat{\bf w}(g)\,\tag{4.31}$$

is large using branching processes. Note that the difference between ΓˆT
h
(f) and Be,f (defined in
(4.21)) is that the latter does not include the weight of heads that are paired in the out-phase.

Consider the marked branching process Xt with offspring distribution η
↑ = η
↑(β), where η =
Dout and β < 1/4 will be fixed later. We define ˆν
↑, Hˆ ↑, φ
↑(a), a
↑
0 for η
↑ analogously to ˆν, Hˆ ,
φ(a) and a0 for η (see (2.8), (2.13) and (2.58)). It is easy to verify that ˆν
↑ = (1 + o(1))ˆν, Hˆ ↑ =
(1 + o(1))Hˆ and φ
↑(a) = (1 + o(1))φ(a). By continuity of φ(a), we have φ
↑(a
↑
0
) = (1 + o(1))φ(a0).

So Hˆ ↑/φ↑(a
↑ 0
) = (1 + o(1))H/φ ˆ (a0). Define γ
↑:= n
−(1+ε/2)Hˆ ↑/φ↑(a
↑
0
) > γ.

Let E1(f) := [ΓˆT
h
(f) *< ωγ/*2] and E2(f) := [t
−
ω
(f) < ω]. Recall the definition of Γˆtin (2.63)
and let E1 := [Γˆtω+h− *< ωγ/*2] and E2 := [tω < ω]. Let E
↑
1
:= [Γˆtω+h− *< ωγ*↑/2] and note that E1 ⊆ E
↑
1
.

Corollary 2.9 with p = n
−(1+ε/2) shows that

$$\mathbb{P}\left\{(G_{t\omega}(\gamma^{\dagger}))^{c}\cap E_{2}\right\}\leq\mathbb{P}\left\{(G_{t\omega}(\gamma^{\dagger}))^{c}\cap[t\omega<\infty]\right\}=O(n^{-(1+\varepsilon/4)})\;.$$

So using Proposition 2.10

$$\mathbb{P}\left\{E_{1}^{\dagger}\cap E_{2}\right\}\leq\mathbb{P}\left\{E_{1}^{\dagger}\cap E_{2}\cap G_{t_{\omega}}(\gamma^{\dagger})\right\}+\mathbb{P}\left\{(G_{t_{\omega}}(\gamma^{\dagger}))^{c}\cap E_{2}\right\}$$ $$\leq\sum_{t_{0}=1}^{\omega}\mathbb{P}\left\{E_{1}^{\dagger}\right|\left[t_{\omega}=t_{0}\right]\cap G_{t_{0}}(\gamma)\right\}\mathbb{P}\left\{t_{\omega}=t_{0}\mid t_{\omega}<\omega\right\}+O(n^{-(1+\varepsilon/4)})\tag{4.33}$$ $$=e^{-\gamma\omega\omega^{1/3}}+O(n^{-(1+\varepsilon/4)})=O(n^{-(1+\varepsilon/4)})\;.$$
$$(4.32)$$

Let T be a marked incomplete tree of height ω + h
− ≥ h. Recall that p(T) is the number of paired nodes in T and that T
−
f
(i) is the marked incomplete tree constructed after the i-th pairing in the graph exploration process. Conditional on [T
−
f
(p(T)) = T], the events E1(f) and E2(f) are measurable. Let T be the set of incomplete trees T of height ω + h
− that satisfy E1(f) ∩ E2(f).

Note that p(T) ≤ κ
− = O(n α) for a constant α > 0 as small as needed. Thus, by Lemma 3.1 with β = 10α and (4.33), we have

$$\mathbb{P}\left\{E_{1}(f)\cap E_{2}(f)\right\}=\mathbb{P}\left\{T_{f}^{-}\in\mathcal{T}\right\}$$ $$\leq(1+o(1))\mathbb{P}\left\{\mathrm{GW}_{\eta^{\dagger}}\in\mathcal{T}\right\}$$ $$=(1+o(1))\mathbb{P}\left\{E_{1}\cap E_{2}\right\}\tag{4.34}$$ $$\leq(1+o(1))\mathbb{P}\left\{E_{1}^{\dagger}\cap E_{2}\right\}$$ $$=O(n^{-(1+\epsilon/4)})\.$$

27 By Lemma 2.7, we have P {Ec 2 | tω < ∞} = o(n
−2). Applying Lemma 3.1 similarly as before shows that P {(E2(f))c| t
−
ω
(f) < ∞} = o(n
−2). Therefore, it follows from (4.34) that

$$\mathbb{P}\left\{[\hat{\Gamma}_{h}^{\mathbf{T}}(f)\geq\omega\gamma/2]\cup[t_{\omega}^{-}(f)=\infty]\right\}=1-O(n^{-(1+\varepsilon/2)})\;.$$

Thus, by applying the union bound first over f ∈ E− and then over e ∈ E−, we have

$$\mathbb{P}\left\{\mathcal{A}_{2}^{c}\right\}=o(1)+\sum_{e,f\in\mathcal{E}^{-}}\mathbb{P}\left\{\left[B_{e,f}<\frac{\omega\gamma}{4}\right]\cap\left[\hat{\Gamma}_{h}^{\mathrm{T}}(f)\geq\frac{\omega\gamma}{2}\right]\right\}.\tag{4.36}$$

Let σ
− be a partial pairing of the at most κ
− = o(n) half-edges that expose N
−
≤h
(f). We now perform the out-phase to construct the tree T
+
econditional on σ
−. Recall that during the outphase at most κ
+ = o(n) edges are formed. Thus,

$$\mathbb{E}\left[B_{e,f}\ \big{|}\ \sigma^{-}\right]=\sum_{g\in\mathcal{N}_{h}^{-}(f)}\hat{\mathbf{w}}(g)\mathbb{E}\left[\mathbf{1}_{g\in\mathcal{G}}\ \big{|}\ \sigma^{-}\right]$$ $$\geq\left(1-\frac{\kappa^{+}}{m-\kappa^{+}-\kappa^{-}}\right)\sum_{g\in\mathcal{N}_{h}^{-}(f)}\hat{\mathbf{w}}(g)\tag{1}$$ $$=(1+o(1))\hat{\Gamma}_{h}^{\mathbf{T}}(f)\.$$
$$(4.35)$$
$$(4.37)$$
$$(4.38)$$
(4.39)  $\huge\square$  . 
Using (4.19), an application of Azuma's inequality (see [21, Chapter 11]) to Be,f determined by the random vector (wˆ (g)1g∈G)g∈N −
h
(f)
implies that

$$\mathbb{P}\left\{B_{e,f}<\frac{\tilde{\Gamma}_{h}^{\mathsf{T}}(f)}{2}\ \bigg{|}\ \sigma^{-}\right\}\leq\exp\left\{-2\frac{(1+o(1)\tilde{\Gamma}_{h}^{\mathsf{T}}(f)^{2}}{\sum_{g\in\mathcal{N}_{h}^{\mathsf{T}}(f)}\mathsf{W}(g)^{2}}\right\}\leq\exp\left\{-2\gamma^{-1}n^{2\varepsilon}\tilde{\Gamma}_{h}^{\mathsf{T}}(f)\right\}\.$$  Since the event $[\tilde{\Gamma}_{h}^{\mathsf{T}}(f)>\frac{\omega_{2}}{2}]$ is measurable with respect to $\sigma^{-}$, we have 
o. (4.38)
$$\mathbb{P}\left\{\left[B_{e,f}<\frac{\omega\gamma}{4}\right]\cap\left[\mathbf{\hat{r}}_{h}^{\mathbf{T}}(f)>\frac{\omega\gamma}{2}\right]\right\}\leq\exp\left\{-2\gamma^{-1}n^{2e}\left(\frac{\omega\gamma}{2}\right)\right\}\leq\exp\left\{-\omega n^{2e}\right\}\ .$$  which is (10.2) of the above $\omega$-function.  
Putting this into (4.36) finishes the proof.

## 4.5 Upper Bound For Π E

0 Let ε > 0 be small enough. Let a0 be the minimiser of φ(a), defined as in (1.10). Let

$$h_{1}:=\frac{1-\varepsilon}{a_{0}\phi(a_{0})}\log n\;,\qquad h_{2}:=\log_{\nu}\omega=O(\log\log n)\;.$$

In this subsection we set γ := n
−(1−ε)Hˆ −/φ(a0). Let Γt(f) be a graph analogue of Γt (see Section 2)

defined by
$$\Gamma_{t}(f):=\sum_{g\in{\mathcal{N}}_{t}^{-}(f)}P^{t}(g,f)\;.$$
t(*g, f*) . (4.41)
Recall the definition of w(g) in (4.16). Define

$$\Gamma_{t}^{\mathbf{T}}(f):=\sum_{g\in{\mathcal{N}}_{t}^{-}(f)}\mathbf{w}(g)\;.$$
$$(4.40)$$
$$(4.42)$$
$$28$$

For any subgraph G with vertex set a subset of [n], let TX(G) := |E(G)| − (|V (G)| − 1) denote the excess of edges of G comparing to the case when it induces a tree.

For f ∈ E−, consider the following events:

$$E_{0}(f)=[t_{\omega}^{-}(f)\leq h_{1}+h_{2}]\;,\;\;\;\;\;E_{1}(f)=[0<\Gamma_{h_{1}}^{T}(f)<\gamma]\;,\tag{4.43}$$ $$E_{2}(f)=[\text{TX}({\cal N}_{\leq h_{1}}^{-}(f))=0]\;,\;\;\;F(f)=\bigcap_{r=1}^{h_{1}}[0<|{\cal N}_{r}^{-}(f)|<\omega]\;.$$
$$(f)\cap F(f).$$
$$(4.44)$$
$\left(4.45\right)^2$
Write A(f) = E0(f) ∩ E1(f) ∩ E2(f) ∩ F(f).

The proof in this section uses T
−
f
, the tree constructed in Subsection 3.1, stopping once all heads at distance h1 + h2 to f have become active. During the construction of T
−
f we deterministically expose at most κ
− 1
:= ωh1 + ω(∆−)
h2+1 = O(logC n) pairings for some constant C.

Proposition 4.8. For all f ∈ E−*, we have*

$$\mathbb{P}\left\{{\mathcal{A}}(f)\right\}=n^{-1+\varepsilon+o(1)}\ .$$
$F=\bigotimes_{k=1}^{h_1}[0,\cdot,X_{k+1}]$
$$(4.46)^{\frac{1}{2}}$$
−1+ε+o(1). (4.44)
Proof. Consider E0 = [tω ≤ h1 + h2], E1 = [0 < Γh1 < γ] and F =Th1
r=1[0 < Xr < ω] to be the
corresponding events on the marked branching process (Xr)r≥0 with offspring distribution η
↓(β)
with η = Dout and an arbitrary β ∈ (0, 1/4) to be determined later. With a = a0 and t = h1,
Theorem 2.5 implies that
$$\mathbb{P}\left\{E_{1}\cap F\right\}\geq n^{-1+\varepsilon+o(1)}\ .$$
−1+ε+o(1). (4.45)
When F happens, there is at least one individual in generation h1. Thus, for some constant c,

$$\mathbb{P}\left\{E_{0}\mid E_{1}\cap F\right\}\geq\mathbb{P}\left\{X_{h_{2}+h_{1}}\geq\omega\mid X_{h_{1}}>0\right\}\geq\mathbb{P}\left\{X_{h_{2}}\geq\omega\right\}>c\;,$$

where in the last inequality we used that ν
−hXh converges to an absolutely continuous random variable, and h2 = logν ω. Therefore, P {E0 ∩ E1 ∩ F} ≥ n
−1+ε+o(1).

Conditional on [T
−
f
(p(T)) = T], the event E0(f) ∩ E1(f) ∩ F(f) is measurable. Let T be the set of incomplete trees T that satisfy E0(f) ∩ E1(f) ∩ F(f). Using Lemma 3.1 we have

$$\mathbb{P}\left\{E_{0}(f)\cap E_{1}(f)\cap F(f)\right\}\geq(1+o(1))\mathbb{P}\left\{\mathrm{GW}_{\eta^{i}}\in\mathcal{T}\right\}\geq n^{-1+\varepsilon+o(1)}\;,$$
−1+ε+o(1), (4.47)
where we used that the parameters ˆν
↓, Hˆ ↓, φ
↓(a
↓
0
) are well-approximated by ˆν, Hˆ , φ(a0), as in the proof of Lemma 4.7.

We next bound the probability of (E2(f))c ∩ F(f). Recall that TX(N
−
≤h1
(f)) is the number of collisions; that is, steps in the exploration process where e
+
i ∈ A+
i−1
. At step i, the probability of a collision is at most ∆+i m−i+1 . So, provided ` ≤ m/2, the number of collisions in the first ` steps is dominated by a binomial random variable with parameters (`, 2*`M/m*). Note that the probability of (E2(f))c ∩F(f) is bounded from above by the probability of at least one collision happens when running the exploration process for at most ` = (ω − 1)h1 = O(log7 n) steps. Thus we have

$$\mathbb{P}\left\{(E_{2}(f))^{c}\cap F(f)\right\}=O(\ell^{2}/m)=n^{-1+o(1)}\ .$$
−1+o(1). (4.48)
Thus, we obtain the lower bound in (4.44):
P {A(f)} = P {E0(f) ∩ E1(f) ∩ E2(f) ∩ F(f)}

$$(4.47)$$
$$(4.48)^{\frac{1}{2}}$$
$\geq\mathbb{P}\left\{E_{0}(f)\cap E_{1}(f)\cap F(f)\right\}-\mathbb{P}\left\{(E_{2}(f))^{c}\cap F(f)\right\}\geq n^{-1+\varepsilon+o(1)}$.  
For the upper bound in (4.44), an application of Theorem 2.6 with offspring η
↑(β), a = a
↑ 0 and t = h1, and Lemma 3.1, as in the proof of Lemma 4.7, gives that

$$\mathbb{P}\left\{\mathcal{A}(f)\right\}\leq\mathbb{P}\left\{E_{1}(f)\cap F(f)\right\}$$ $$\leq(1+o(1))\mathbb{P}\left\{E_{1}\cap F\right\}$$ $$\leq\mathbb{P}\left\{(G_{t}(e^{-a\hat{H}t}))^{c}\cap[0<X_{t}<\omega]\right\}$$ $$\leq n^{-1+\varepsilon+o(1)}\.$$
$$\begin{array}{c}{{(4.49)}}\\ {{\square}}\end{array}$$

We will show that Z :=Pf∈E− 1A(f)satisfies Z > 0 whp, using a second moment calculation.

As m ≥ n, Proposition 4.8 implies that E[Z] ≥ n ε/2. For a partial pairing σ0, we write f ∈ Im(σ0)
if f has been paired by σ0. For f1, f2 ∈ E− with f1 6= f2, we have

$$\mathbb{P}\left\{{\mathcal{A}}(f_{1})\cap{\mathcal{A}}(f_{2})\right\}\leq\sum_{\sigma_{0}\in{\mathcal{A}}(f_{1})}\mathbb{P}\left\{\sigma_{0}\right\}\left(\mathbf{1}_{f_{2}\in I m(\sigma_{0})}+\mathbf{1}_{f_{2}\notin I m(\sigma_{0})}\mathbb{P}\left\{{\mathcal{A}}(f_{2})\mid\sigma_{0}\right\}\right)$$
$$\leq\mathbb{P}\left\{\mathcal{A}(f_{1})\right\}\left(O\left(\frac{\log^{C}n}{n}\right)+\operatorname*{max}_{\begin{array}{l}{{\sigma_{0}\in\mathcal{A}(f_{1})}}\\ {{f_{2}\notin I m(\sigma_{0})}}\end{array}}\mathbb{P}\left\{\mathcal{A}(f_{2})\mid\sigma_{0}\right\}\right)\;,$$
$$(4.50)$$
$$(4.51)$$
 , (4.51)
where we used that at most κ
−
1 = O(logC n) edges need to be exposed to determine A(f1).

We briefly describe how to compute P {A(f2) | σ0} for f2 ∈/ Im(σ0). Start the exploration process in Subsection 3.1 from f2 with one modification: if e
−
ihas already been paired in σ0, then we choose e
+
iaccording to σ0 instead of uniformly at random. Since at most κ
−
1 half-edges need to be paired, the probability of [N≤h1+h2
(f2) ∩ Im(σ0) 6= ∅] is at most O((κ
− 1
)
2/n). This implies that

$$\mathbb{P}\left\{{\mathcal{A}}(f_{2})\mid\sigma_{0}\right\}\leq\mathbb{P}\left\{{\mathcal{A}}(f_{2})\right\}+O\left({\frac{(\kappa_{1}^{-})^{2}}{n}}\right)\ .$$
$$(4.52)$$
$$(4.53)$$

By Proposition 4.8, we obtain

$$\mathbb{P}\left\{{\mathcal{A}}(f_{1})\cap{\mathcal{A}}(f_{2})\right\}\leq\mathbb{P}\left\{{\mathcal{A}}(f_{1})\right\}\left(O\left({\frac{(\kappa_{1}^{-})^{2}}{n}}\right)+\mathbb{P}\left\{{\mathcal{A}}(f_{2})\right\}\right)\leq n^{-2+3\varepsilon}\;.$$

So E[Z
2] = (1 + o(1))E[Z]
2, and Chebyshev's inequality implies that whp [Z > 0].

Under [Z > 0], let f0 be such that A(f0) holds. As E0(f0) holds, we have f0 ∈ E−
0
. Moreover, E1(f0) ∩ E2(f0) implies that Pg∈E− P
h1 (*g, f*0) = Γh1
(f0) = ΓT
h1
(f0) < γ. Moreover, Remark 1.5 implies that π e(g) ≤ n
−1+o(1) for any g ∈ E−. By stationarity at time h1, we obtain

$$\pi_{0}^{\mathsf{e}}\leq\pi^{\mathsf{e}}(f_{0})=\sum_{g\in\mathcal{E}^{-}}P^{h_{1}}(g,f_{0})\pi^{\mathsf{e}}(g)\leq n^{-1-{\frac{(1-\varepsilon)\hat{H}^{-}}{\phi(a_{0})}}+o(1)}\;,$$

for any ε > 0 and the upper bound in Theorem 1.1 follows.

## 4.6 The Proof Of Remark 1.8

To show the upper bound in (1.16) in Remark 1.8, we can follow the line of arguments in Subsection 4.3 and Subsection 4.4. Here we briefly sketch the changes needed for it to work. For α ∈ [0, Hˆ −/φ (a0)], let γα := n
−α and β =
αφ(a0)
Hˆ − ≤ 1. By Corollary 2.9 with p = n
−β, we have

$$\mathbb{P}\left\{(G_{t_{\omega}}(\gamma_{\alpha}))^{c}\cap[t_{\omega}<\infty]\right\}\leq n^{-\beta+o(1)}\ .$$
−β+o(1). (4.54)
Let

$$\mathcal{Y}_{f}^{\alpha}:=\bigcap_{e\in\mathcal{E}^{-}}\left[\left[A_{e,f}\geq\frac{1}{2}\right]\cap\left[B_{e,f}\geq\frac{\omega\gamma_{\alpha}}{4}\right]\right]\;.$$

Then by the same argument of Lemma 4.6 using (4.54), we have P(Y
α f
)
c ∩ [t
− ω
(f) < ∞]	≤ n
−β+o(1). (4.56)
Let B(f) :=-0 < πe(f) <
γα 2n
. It follows from Proposition 4.1 and Lemma 4.5 that

$$\begin{split}\mathbb{P}\left\{\mathcal{B}(f)\right\}&=(1+o(1))\mathbb{P}\left\{\left[\pi^{\alpha}(f)<\frac{\gamma_{\alpha}}{2n}\right]\cap\left[\tau_{\omega}^{-}(f)<\infty\right]\right\}\\ &\leq(1+o(1))\left(\mathbb{P}\left\{\left[\pi^{\alpha}(f)<\frac{\gamma_{\alpha}}{2n}\right]\cap\mathcal{Y}_{f}^{\beta}\right\}+\mathbb{P}\left\{(\mathcal{Y})^{e}\cap\left[\tau_{\omega}^{-}(f)<\infty\right]\right\}\right)\\ &=(1+o(1))\mathbb{P}\left\{\cup_{e\in\mathcal{E}^{-}}\left[\hat{Pr}^{\prime}(f)(e,f)<\frac{\gamma_{\alpha}}{2n}\right]\cap\mathcal{Y}_{f}^{\beta}\right\}+n^{-\beta+o(1)}\\ &=n^{-\beta+o(1)}\,,\end{split}\tag{4.57}$$
$$(4.54)$$
$$(4.55)$$
$$(4.56)$$

$$(4.58)$$

where in the last step we have used Lemma 4.5.

Recall the definition of A(f) below (4.43). Define Aα(f) analogously with h1 replaced by h1,α = βh1 and γ replaced by γα. Then by the same argument of Proposition 4.8, we have

$$\mathbb{P}\left\{{\mathcal{B}}(f)\right\}\geq(1+o(1))\mathbb{P}\left\{{\mathcal{A}}_{\alpha}(f)\right\}\geq n^{-\beta+o(1)}\;,$$
−β+o(1), (4.58)
and Remark 1.8 follows.

## 5 Applications 5.1 Hitting And Cover Times

We now prove Theorem 1.9, i.e., whp the maximal hitting and the cover time of G~n are both n 1+Hˆ −/φ(a0)+o(1). Clearly, τ hit ≤ τ cov, so it suffices to lower bound τ hit and upper bound τ cov.

For the lower bound, let C < (1 + Hˆ −/φ(a0)) be a constant. Then by Theorem 1.1, we have πmin ≤ n
−C whp. Recall the definition of τx(y) in Subsection 1.2. The *returning time* to x ∈ [n]
is τ
+
x:= inf{t ≥ 1 : Zt = *x, Z*0 = x}. By the well-known relation between the expected returning time and the stationary distribution, we have whp

$$\max_{x\in[n]}\mathbb{E}\left[\tau_{x}^{+}\ \big{|}\ \bar{G}_{n}\right]=\max_{x\in[n]}\frac{1}{\pi(x)}=\frac{1}{\pi_{\min}}\geq n^{C}.\tag{5.1}$$
Thus, whp there exists a vertex $x_0\in[n]$ such that $\mathbb{E}\left[\tau_{x_0}^+\ \middle|\ \tilde{G}_n\right]\geq n^C$, which implies that. 
$$n^{C}\leq\mathbb{E}\left[\tau_{x_{0}}^{+}\ \Big{|}\ \tilde{\mathcal{G}}_{n}\right]=1+\frac{1}{d_{x_{0}}^{+}}\sum_{y\in\mathcal{N}_{\leq1}^{+}(x_{0})}m(x_{0},y)\mathbb{E}\left[\tau_{y}(x_{0})\ \Big{|}\ \tilde{\mathcal{G}}_{n}\right]\,\tag{5.2}$$
$$(5.3)$$

where m(*x, y*) is the multiplicity of the directed edge (*x, y*) in G~n. Thus, whp there exists two
vertices x0 and y0 such that
$$\mathbb{E}\left[\tau_{y_{0}}(x_{0})\;\Big\vert\;\vec{\mathbb{G}}_{n}\right]\geq n^{C}-1\;.$$
C − 1 . (5.3)
It follows that τ hit ≥ n C − 1 whp.

For the upper bound, let C > (1 + Hˆ −/φ(a0)) be a fixed constant and let τ = ω 2 = log12 n.

Recall the definition of V0 in Proposition 4.1. Lemma 4.4 implies that whp for all x ∈ [n] and y ∈ V0, there exists τ (y) ≤ τ (by (4.12)) such that P
τ(y)(*x, y*) ≥ n
−C,. So the probability to hit y in at most τ steps, which we call a try, is at least n
−C uniformly for any starting point x. Thus, the number of tries needed to hit y is stochastically dominated by a geometric random variable with success probability n
−C. It follows that whp

$$\tau_{\rm hit}=\max_{\begin{subarray}{c}x\in[n]\\ y\in{\cal W}_{0}\end{subarray}}\mathbb{E}[\tau_{x}(y)]\leq n^{C}\tau\.$$
$$(5.4)$$

Therefore, by Matthews' bound [20, Theorem 2.6], we have

$$\tau_{\mathrm{cov}}\leq H_{n}\tau_{\mathrm{hit}}=n^{C+o(1)}\ ,$$
$$(5.5)$$
C+o(1), (5.5)
where Hn is the n-th harmonic number.

## 5.2 Explicit Constants For Particular Degree Sequences

In this section we discuss two particular examples where the polynomial exponent can be made explicit.

## 5.2.1 R-Out Digraph

For any integer r ≥ 2, an r-out digraph Dn,r is a random directed graph with n vertices in which each vertex chooses r out-neighbours uniformly at random. It is used as a model for studying uniform random Deterministic Finite Automata [7]. For r ≥ 2, Addario-Berry, Balle, Perarnau [1]
showed that in Dn,r, whp, πmin = n
−(1+log(r)/(sr−log r)+o(1)), (5.6)

$\mathbb{T}_{\min}=n^{-(1+\mathbb{I})}$
where s is the largest solution of 1 − s = e
−sr.

Although in Dn,r the in-degrees are random, events that hold whp in G~n also holds whp in Dn,r, assuming that D− (the in-degree of a uniform random vertex in G~n) converges to a Poisson distribution with mean r whereas the D+ = r ≥ 2 almost surely. It is an exercise to check that whp the maximum in-degree of Dn,p has order log n log log n
. A careful inspection of the proof of Theorem 1.1 shows that the bounded maximum degree condition can be relaxed to ∆± = o(log n). Therefore, the conclusion of Theorem 1.1 holds in this setting. As P {D+ = r} = 1, we have I(aHˆ −) = ∞ for any a 6= 1, so a0 = 1 and πmin = n
−(1+|log ˆν−|+o(1)), which coincides with (5.6).

$$r)+o(1))\quad,$$
$\left(5.6\right)^{2}$
$\therefore\frac{-8r}{r}$ . 

## 5.2.2 A Toy Example

Here we show how the explicit value for πmin can be computed for a simple distribution, providing an example where a0 6= 1. Consider a degree distribution D defined by

$$\mathbb{P}\left\{D=(0,2)\right\}=\mathbb{P}\left\{D=(0,3)\right\}=\mathbb{P}\left\{D=(5,0)\right\}=\mathbb{P}\left\{D=(5,2)\right\}=\frac{1}{4}\;.\tag{5.7}$$

As D+ is uniform on {2, 3} and is independent from D−, we have D˜ +
out = log 2 + log(3/2)X, where X is a Bernoulli random variable with probability p = 3/5. The large deviation rate function for a Bernoulli random variable with probability p is IBe(z) = z log zp
+ (1 − z) log 1−z 1−p for z ∈ [0, 1]
(see, e.g., [16, Exercise 2.2.23]). Thus, we have I(z) = IBe (log(3/2))−1(z − log 2).

With the help of interval arithmetic libraries [24], we get

$$\hat{H}^{-}\doteq0.936426\,\quad\hat{\nu}\doteq0.181095\,\quad a_{0}\doteq1.06671\,\quad\phi(a_{0})\doteq1.65129\,\quad1+\frac{\hat{H}^{-}}{\phi(a_{0})}\doteq1.56708\,\tag{5.8}$$

with errors guaranteed to be at most 10−6 by the algorithm. As shown in Figure 2, the function φ(a) attains minimum at a0 > 1. In particular, the vertex that is *hardest* to hit has a "thin" in-neighbourhood which is of 97.1% of the length of the longest such "thin" in-neighbourhoods. In other words, it is not the vertex that is furthest away from others that is hardest to find.

 

Acknowledgements. We would like to thanks Pietro Caputo and Matteo Quattropani for insightful discussions on the topic.

## References

[1] L. Addario-Berry, B. Balle, and G. Perarnau. Diameter and Stationary Distribution of Random r-Out Digraphs. *The Electronic Journal of Combinatorics*, pages P3.28–P3.28, Aug. 2020. ISSN
1077-8926. doi: 10/ghd74q.
[2] H. Amini. Bootstrap Percolation in Living Neural Networks. *J Stat Phys*, 141(3):459–475, Nov. 2010. ISSN 1572-9613. doi: 10/c53hx4.

[3] K. B. Athreya and P. E. Ney. *Branching Processes*. Grundlehren Der Mathematischen Wissenschaften. Springer-Verlag, Berlin Heidelberg, 1972. doi: 10/dft4.

[4] J. Blanchet and A. Stauffer. Characterizing optimal sampling of binary contingency tables via the configuration model. *Random Structures & Algorithms*, 42(2):159–184, 2013.

doi: 10/f4mtxh.

[5] C. Bordenave, P. Caputo, and J. Salez. Random walk on sparse random digraphs. *Probab.*
Theory Relat. Fields, 170(3):933–960, Apr. 2018. ISSN 1432-2064. doi: 10/gc8nxk.

[6] C. Bordenave, P. Caputo, and J. Salez. Cutoff at the "entropic time" for sparse Markov chains.

Probab. Theory Relat. Fields, 173(1):261–292, Feb. 2019. ISSN 1432-2064. doi: 10/ghcrhr.

[7] X. S. Cai and L. Devroye. The graph structure of a deterministic automaton chosen at random.

Random Structures & Algorithms, 51(3):428–458, 2017. ISSN 1098-2418. doi: 10/gbtqgb.

[8] X. S. Cai and G. Perarnau. The giant component of the directed configuration model revisited.

arXiv:2004.04998 [cs, math], Apr. 2020. URL http://arxiv.org/abs/2004.04998.

[9] X. S. Cai and G. Perarnau. The diameter of the directed configuration model. *arXiv:2003.04965*
[cs, math], Mar. 2020. URL http://arxiv.org/abs/2003.04965.

[10] P. Caputo and M. Quattropani. Mixing time of PageRank surfers on sparse random digraphs.

arXiv:1905.04993 [math], July 2020. URL http://arxiv.org/abs/1905.04993.

[11] P. Caputo and M. Quattropani. Stationary distribution and cover time of sparse directed configuration models. *Probab. Theory Relat. Fields*, Aug. 2020. ISSN 1432-2064. doi: 10/ghd74v.

[12] S. Chatterjee. Stein's method for concentration inequalities. *Probab. Theory Relat. Fields*, 138
(1-2):305–321, Feb. 2007. ISSN 0178-8051, 1432-2064. doi: 10/fm2x4r.

[13] N. Chen, N. Litvak, and M. Olvera-Cravioto. Generalized PageRank on directed configuration networks. *Random Structures & Algorithms*, 51(2):237–274, 2017. ISSN 1098-2418.

doi: 10/gbrth6.

[14] C. Cooper and A. Frieze. The Size of the Largest Strongly Connected Component of a Random Digraph with a Given Degree Sequence. *Combinatorics, Probability and Computing*, 13(3):
319–337, May 2004. doi: 10/cn8q5j.

[15] C. Cooper and A. Frieze. Stationary distribution and cover time of random walks on random digraphs. *J. Comb. Theory Ser. B*, 102(2):329–362, Mar. 2012. ISSN 0095-8956. doi: 10/cv9wbh.

[16] A. Dembo and O. Zeitouni. *Large Deviations Techniques and Applications*. Stochastic Modelling and Applied Probability. Springer-Verlag, Berlin Heidelberg, second edition, 2010.

doi: 10.1007/978-3-642-03311-7.

[17] A. Graf. *On the Strongly Connected Components of Random Directed Graphs with given Degree* Sequences. PhD thesis, University of Waterloo, 2016. URL http://hdl.handle.net/10012/
10681.

[18] S. Janson. The probability that a random multigraph is simple. Combinatorics, Probability and Computing, 18(1-2):205–225, 2009. doi: 10/bg4m2c.

[19] H. Li. Attack Vulnerability of Online Social Networks. In *2018 37th Chinese Control Conference (CCC)*, pages 1051–1056, July 2018. doi: 10/ggh2kg.

[20] P. Matthews. Covering Problems for Brownian Motion on Spheres. *Ann. Probab.*, 16(1):
189–199, Jan. 1988. ISSN 0091-1798, 2168-894X. doi: 10/c8q2r8.

[21] M. Molloy and B. Reed. *Graph Colouring and the Probabilistic Method*. Algorithms and Combinatorics. Springer-Verlag, Berlin Heidelberg, 2002. doi: 10.1007/978-3-642-04016-0.

[22] V. Petrov. *Sums of Independent Random Variables*. Ergebnisse Der Mathematik Und Ihrer Grenzgebiete. 2. Folge. Springer-Verlag, Berlin Heidelberg, 1975.

doi: 10.1007/978-3-642-65809-9.

[23] O. Riordan and N. Wormald. The diameter of sparse random graphs. Combin. Probab.

Comput., 19(5-6):835–926, 2010. ISSN 0963-5483. doi: 10/dgp6hh.

[24] D. P. Sanders, L. Benet, K. Agarwal, E. Gupta, B. Richard, M. Forets, yashrajgupta, E. Hanson, B. van Dyk, C. Rackauckas, S. Miclua-Cˆampeanu, T. Koolen, C. Wormell, F. A. V´azquez, J. Grawitter, J. TagBot, K. O'Bryant, K. Carlsson, M. Piibeleht, Reno, R. Deits, S. Olver, T. Holy, kalmarek, and matsueushi. JuliaIntervals/IntervalArithmetic.jl: V0.17.5. Zenodo, June 2020. URL https://github.com/JuliaIntervals/IntervalArithmetic.jl.

[25] R. van der Hofstad. *Random Graphs and Complex Networks*, volume 1 of Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, England, 2016. doi: 10.1017/9781316779422.

[26] P. van der Hoorn and M. Olvera-Cravioto. Typical distances in the directed configuration model. *Ann. Appl. Probab.*, 28(3):1739–1792, June 2018. ISSN 1050-5164, 2168-8737.

doi: 10/ggh2ch.